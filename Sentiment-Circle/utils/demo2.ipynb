{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import random\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "import datasets\n",
    "import numpy as np\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    EvalPrediction,\n",
    "    HfArgumentParser,\n",
    "    PrinterCallback,\n",
    "    Trainer,\n",
    "    AutoModel\n",
    ")\n",
    "from transformers import TrainingArguments as HFTrainingArguments\n",
    "from transformers import default_data_collator, set_seed\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "\n",
    "from progress_logger import LogCallback\n",
    "from sts2.modeling_utils import DataCollatorForBiEncoder, get_model\n",
    "import torch\n",
    "from sts2.dataset_preprocessing import get_preprocessing_function, parse_dict, batch_get_preprocessing_function\n",
    "from sts2.modeling_encoders import BiEncoderForClassification\n",
    "from sts2.clf_trainer import CustomTrainer\n",
    "from sts2.metrics import compute_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# カスタムモデルのテスト"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初期化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AutoConfig' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# デフォルトのconfig, ModelArgumentsの設定\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# model_path = \"bert-base-uncased\"\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# model_path = \"mixedbread-ai/mxbai-embed-large-v1\"\u001b[39;00m\n\u001b[32m      4\u001b[39m model_path = \u001b[33m\"\u001b[39m\u001b[33mQwen/Qwen3-Embedding-0.6B\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m config = \u001b[43mAutoConfig\u001b[49m.from_pretrained(model_path)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(config)\n\u001b[32m      7\u001b[39m config.model_name_or_path = model_path  \u001b[38;5;66;03m# ここを追加\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'AutoConfig' is not defined"
     ]
    }
   ],
   "source": [
    "# デフォルトのconfig, ModelArgumentsの設定\n",
    "# model_path = \"bert-base-uncased\"\n",
    "# model_path = \"mixedbread-ai/mxbai-embed-large-v1\"\n",
    "model_path = \"Qwen/Qwen3-Embedding-0.6B\"\n",
    "config = AutoConfig.from_pretrained(model_path)\n",
    "print(config)\n",
    "config.model_name_or_path = model_path  # ここを追加\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "config.attn_implementation = \"sdpa\" # sdpaのエラー対策\n",
    "# config.device_map = \"auto\"\n",
    "config.device_map = \"cuda\"\n",
    "config.config_name = None\n",
    "config.tokenizer_name = None\n",
    "config.cache_dir = None\n",
    "config.use_fast_tokenizer = True\n",
    "config.model_revision = \"main\"\n",
    "config.use_auth_token = None\n",
    "config.torch_dtype= \"bfloat16\"  # データ型をbfloat16に設定\n",
    "# config.torch_dtype = \"float16\"  # データ型をfloat16に設定\n",
    "# config.attn_implementation = \"flash_attention_2\"  \n",
    "config.objective = \"mse\"\n",
    "config.encoding_type = \"bi_encoder\"\n",
    "config.pooler_type = \"last\"\n",
    "# config.pooler_type = \"avg\"\n",
    "config.freeze_encoder = True\n",
    "config.transform = False\n",
    "config.triencoder_head = \"hadamard\"\n",
    "config.classifier_save_directory = \"./output_test\"\n",
    "\n",
    "clf_configs = {\n",
    "# \"Concepts\": {\"type\": \"linear\", \"objective\": \"regression\", \"distance\":\"dot_product\",\"output_dim\": 128, \"dropout\": 0.1, \"layer\": 9},\n",
    "# \"Frames\": {\"type\": \"mlp2\", \"objective\": \"regression\", \"distance\":\"cosine\",\"intermediate_dim\": 512, \"bottleneck_dim\": 256, \"output_dim\": 64, \"dropout\": 0.1, \"layer\": 10},\n",
    "\"apt_label\": {\"type\": \"linear\", \"objective\": \"binary_classification\", \"distance\":\"cosine\", \"output_dim\": 128, \"dropout\": 0.1, \"layer\": 11},\n",
    "\"emotion\": {\"type\": \"contrastive_logit\", \"objective\": \"contrastive_logit\", \"distance\": \"cosine\", \"intermediate_dim\": 256, \"output_dim\": 6,  \"dropout\": 0.1, \"layer\": 12}\n",
    "# \"argpairs_score\": {\"type\":\"linear\",\"objective\":\"regression\",\"distance\":\"cosine\",\"output_dim\":256,\"dropout\":0.1,\"layer\": 23},\n",
    "# \"bws_score\": {\"type\":\"linear\",\"objective\":\"regression\",\"distance\":\"cosine\",\"output_dim\":256,\"dropout\":0.1,\"layer\": 23}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtype: torch.bfloat16, device: cuda:0\n",
      "勾配計算を行うパラメータ数： 0 / 316\n"
     ]
    }
   ],
   "source": [
    "from sts2.modeling_encoders import BiEncoderForClassification\n",
    "model = BiEncoderForClassification(model_config=config, classifier_configs=clf_configs)\n",
    "# 全パラメータを凍結\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 確認\n",
    "print(\"勾配計算を行うパラメータ数：\",\n",
    "      sum(p.requires_grad for p in model.parameters()),\n",
    "      \"/\", len(list(model.parameters())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiEncoderForClassification(\n",
       "  (backbone): Qwen3Model(\n",
       "    (embed_tokens): Embedding(151669, 1024)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen3DecoderLayer(\n",
       "        (self_attn): Qwen3Attention(\n",
       "          (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): Qwen3MLP(\n",
       "          (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "    (rotary_emb): Qwen3RotaryEmbedding()\n",
       "  )\n",
       "  (pooler): Pooler()\n",
       "  (embedding_classifiers): ModuleDict(\n",
       "    (apt_label): LinearLayer(\n",
       "      (linear): Sequential(\n",
       "        (0): Dropout(p=0.1, inplace=False)\n",
       "        (1): Linear(in_features=1024, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (emotion): ContrastiveClassifier(\n",
       "      (embedder): Sequential(\n",
       "        (0): Dropout(p=0.1, inplace=False)\n",
       "        (1): Linear(in_features=1024, out_features=256, bias=True)\n",
       "      )\n",
       "      (classifier): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=6, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.backbone.named_parameters():\n",
    "    if torch.isnan(param).any():\n",
    "        print(f\"{name} に NaN が含まれています\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    32,   3460,  11031,    374,  20327, 151643]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent1 = tokenizer(\"An airplane is taking off\", return_tensors=\"pt\")\n",
    "sent2 = tokenizer(\"A plane is taking off\", return_tensors=\"pt\")\n",
    "sent3 = tokenizer(\"A large plane is landing\", return_tensors=\"pt\")\n",
    "sent1.to(model.device)\n",
    "sent2.to(model.device)\n",
    "sent3.to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'apt_label': tensor([0.6602], device='cuda:0', dtype=torch.bfloat16),\n",
       " 'emotion': tensor([0.7461], device='cuda:0', dtype=torch.bfloat16),\n",
       " 'overall_similarity': tensor([0.9297], device='cuda:0', dtype=torch.bfloat16)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(\n",
    "      input_ids=sent1[\"input_ids\"], \n",
    "      attention_mask=sent1[\"attention_mask\"], \n",
    "      token_type_ids=None, \n",
    "      input_ids_2=sent2[\"input_ids\"],\n",
    "      attention_mask_2=sent2[\"attention_mask\"],\n",
    "      token_type_ids_2=None,\n",
    "      input_ids_3=None,\n",
    "      attention_mask_3=None,\n",
    "      token_type_ids_3=None,\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'apt_label_pos_similarity': tensor([0.9453], device='cuda:0', dtype=torch.bfloat16),\n",
       " 'apt_label_neg_similarity': tensor([0.9258], device='cuda:0', dtype=torch.bfloat16),\n",
       " 'emotion_pos_similarity': tensor([0.9336], device='cuda:0', dtype=torch.bfloat16),\n",
       " 'emotion_neg_similarity': tensor([0.9414], device='cuda:0', dtype=torch.bfloat16),\n",
       " 'emotion_anchor_prob': tensor([[0.0835, 0.1270, 0.2930, 0.1914, 0.1387, 0.1670]], device='cuda:0',\n",
       "        dtype=torch.bfloat16),\n",
       " 'emotion_positive_prob': tensor([[0.0786, 0.1147, 0.3145, 0.1387, 0.1416, 0.2119]], device='cuda:0',\n",
       "        dtype=torch.bfloat16),\n",
       " 'emotion_negative_prob': tensor([[0.1011, 0.1289, 0.3027, 0.1377, 0.1416, 0.1875]], device='cuda:0',\n",
       "        dtype=torch.bfloat16),\n",
       " 'overall_pos_similarity': tensor([0.9336], device='cuda:0', dtype=torch.bfloat16),\n",
       " 'overall_neg_similarity': tensor([0.7383], device='cuda:0', dtype=torch.bfloat16)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(\n",
    "      input_ids=sent1[\"input_ids\"], \n",
    "      attention_mask=sent1[\"attention_mask\"], \n",
    "      token_type_ids=None, \n",
    "      input_ids_2=sent2[\"input_ids\"],\n",
    "      attention_mask_2=sent2[\"attention_mask\"],\n",
    "      token_type_ids_2=None,\n",
    "      input_ids_3=sent3[\"input_ids\"],\n",
    "      attention_mask_3=sent3[\"attention_mask\"],\n",
    "      token_type_ids_3=None\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('argpairs_score', LinearLayer(\n",
       "  (linear): Sequential(\n",
       "    (0): Dropout(p=0.1, inplace=False)\n",
       "    (1): Linear(in_features=1024, out_features=256, bias=True)\n",
       "  )\n",
       ")), ('bws_score', LinearLayer(\n",
       "  (linear): Sequential(\n",
       "    (0): Dropout(p=0.1, inplace=False)\n",
       "    (1): Linear(in_features=1024, out_features=256, bias=True)\n",
       "  )\n",
       "))])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embedding_classifiers.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APT\n",
      "LinearLayer(\n",
      "  (linear): Sequential(\n",
      "    (0): Dropout(p=0.1, inplace=False)\n",
      "    (1): Linear(in_features=2560, out_features=128, bias=True)\n",
      "  )\n",
      ")\n",
      "=====\n"
     ]
    }
   ],
   "source": [
    "for name, classifier in model.embedding_classifiers.items():\n",
    "    print(name)\n",
    "    print(classifier)\n",
    "    print(\"=====\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets1 = load_dataset(\"csv\", data_files=\"/works/data3/users/yama11235/yama11235/SLBERT/utils/data_style/ELSA_joy-sadness-anger-surprise-love-fear_test.csv\")\n",
    "raw_datasets2 = load_dataset(\"csv\", data_files=\"data/APT_train.csv\")\n",
    "raw_datasets = datasets.concatenate_datasets([raw_datasets1[\"train\"].select(range(5)), raw_datasets2[\"train\"].select(range(5))])\n",
    "# raw_datasets1 = load_dataset(\"csv\", data_files=\"/works/data3/users/yama11235/yama11235/SLBERT/utils/data_preprocessed/ArgPairs_test.csv\")\n",
    "# raw_datasets2 = load_dataset(\"csv\", data_files=\"/works/data3/users/yama11235/yama11235/SLBERT/utils/data_preprocessed/BWS_test.csv\")\n",
    "# raw_datasets = datasets.concatenate_datasets([raw_datasets1[\"train\"].select(range(16)), raw_datasets2[\"train\"].select(range(16))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': 'I’m really grateful that my needs have been met and my ideas respected!',\n",
       " 'sentence2': 'As I flipped through some amusing content, I stumbled upon a quirky chart that claimed to reveal how your birth sign could lead you and your partner to the most entertaining zones on your body, a lighthearted insight from astrologer Darryl Gaines.',\n",
       " 'sentence3': 'I feel disgusted really I feel kind of let down',\n",
       " 'emotion': 0,\n",
       " 'apt_label': None}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I’m really grateful that my needs have been met and my ideas respected!',\n",
       " 'In a moment of uncertainty, I felt like a music box that had been roughly handled, plagued by the fear that my melody was now flawed.',\n",
       " 'It hit me like a ton of bricks when I heard Jazmine share her feelings on homosexuality; I never expected that!',\n",
       " 'I seem to have reverted to a prior condition marked by strife with others, in a relentless quest for acknowledgment, while grappling with an overwhelming sense of inadequacy.',\n",
       " \"I try to keep things here in the bol positive and to be perfectly honest I'm not feeling so positive lately\",\n",
       " 'Sgt. ernest bucklew, 33, was coming home from iraq to bury his mother in pennsylvania.',\n",
       " 'Sgt. ernest bucklew, 33, was coming home from iraq to bury his mother in pennsylvania.',\n",
       " 'Sgt. ernest bucklew, 33, was coming home from iraq to bury his mother in pennsylvania.',\n",
       " 'Sgt. ernest bucklew, 33, was coming home from iraq to bury his mother in pennsylvania.',\n",
       " 'Sgt. ernest bucklew, 33, was coming home from iraq to bury his mother in pennsylvania.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"sentence1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_function = batch_get_preprocessing_function(\n",
    "    tokenizer=tokenizer,\n",
    "    sentence1_key=\"sentence1\",\n",
    "    sentence2_key=\"sentence2\",\n",
    "    sentence3_key=\"sentence3\",\n",
    "    sentence3_flag=True,\n",
    "    aspect_key=['emotion', 'apt_label'],\n",
    "    # aspect_key=['argpairs_score', 'bws_score'],\n",
    "    padding=\"max_length\",\n",
    "    max_seq_length=32,\n",
    "    model_args=config,\n",
    "    scale=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10/10 [00:00<00:00, 678.03 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_datasets = raw_datasets.map(\n",
    "    preprocessing_function,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets.column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[40,\n",
       "   4249,\n",
       "   2167,\n",
       "   25195,\n",
       "   429,\n",
       "   847,\n",
       "   3880,\n",
       "   614,\n",
       "   1012,\n",
       "   2270,\n",
       "   323,\n",
       "   847,\n",
       "   6708,\n",
       "   30287,\n",
       "   0,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643],\n",
       "  [641,\n",
       "   264,\n",
       "   4445,\n",
       "   315,\n",
       "   26826,\n",
       "   11,\n",
       "   358,\n",
       "   6476,\n",
       "   1075,\n",
       "   264,\n",
       "   4627,\n",
       "   3745,\n",
       "   429,\n",
       "   1030,\n",
       "   1012,\n",
       "   17267,\n",
       "   17608,\n",
       "   11,\n",
       "   65302,\n",
       "   553,\n",
       "   279,\n",
       "   8679,\n",
       "   429,\n",
       "   847,\n",
       "   61584,\n",
       "   572,\n",
       "   1431,\n",
       "   46908,\n",
       "   13,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643],\n",
       "  [2132,\n",
       "   4201,\n",
       "   752,\n",
       "   1075,\n",
       "   264,\n",
       "   8766,\n",
       "   315,\n",
       "   49037,\n",
       "   979,\n",
       "   358,\n",
       "   6617,\n",
       "   619,\n",
       "   1370,\n",
       "   5967,\n",
       "   4332,\n",
       "   1059,\n",
       "   15650,\n",
       "   389,\n",
       "   52351,\n",
       "   26,\n",
       "   358,\n",
       "   2581,\n",
       "   3601,\n",
       "   429,\n",
       "   0,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643],\n",
       "  [40,\n",
       "   2803,\n",
       "   311,\n",
       "   614,\n",
       "   93593,\n",
       "   311,\n",
       "   264,\n",
       "   4867,\n",
       "   2971,\n",
       "   12864,\n",
       "   553,\n",
       "   96612,\n",
       "   448,\n",
       "   3800,\n",
       "   11,\n",
       "   304,\n",
       "   264,\n",
       "   59035,\n",
       "   2222,\n",
       "   369,\n",
       "   77470,\n",
       "   11,\n",
       "   1393,\n",
       "   89734,\n",
       "   448,\n",
       "   458,\n",
       "   22024,\n",
       "   5530,\n",
       "   315,\n",
       "   39106,\n",
       "   446,\n",
       "   151643],\n",
       "  [40,\n",
       "   1430,\n",
       "   311,\n",
       "   2506,\n",
       "   2513,\n",
       "   1588,\n",
       "   304,\n",
       "   279,\n",
       "   20771,\n",
       "   6785,\n",
       "   323,\n",
       "   311,\n",
       "   387,\n",
       "   13942,\n",
       "   10745,\n",
       "   358,\n",
       "   2776,\n",
       "   537,\n",
       "   8266,\n",
       "   773,\n",
       "   6785,\n",
       "   30345,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643],\n",
       "  [50,\n",
       "   5178,\n",
       "   13,\n",
       "   91929,\n",
       "   477,\n",
       "   22012,\n",
       "   74964,\n",
       "   11,\n",
       "   220,\n",
       "   18,\n",
       "   18,\n",
       "   11,\n",
       "   572,\n",
       "   5001,\n",
       "   2114,\n",
       "   504,\n",
       "   6216,\n",
       "   36306,\n",
       "   311,\n",
       "   55344,\n",
       "   806,\n",
       "   6554,\n",
       "   304,\n",
       "   42949,\n",
       "   18480,\n",
       "   13,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643],\n",
       "  [50,\n",
       "   5178,\n",
       "   13,\n",
       "   91929,\n",
       "   477,\n",
       "   22012,\n",
       "   74964,\n",
       "   11,\n",
       "   220,\n",
       "   18,\n",
       "   18,\n",
       "   11,\n",
       "   572,\n",
       "   5001,\n",
       "   2114,\n",
       "   504,\n",
       "   6216,\n",
       "   36306,\n",
       "   311,\n",
       "   55344,\n",
       "   806,\n",
       "   6554,\n",
       "   304,\n",
       "   42949,\n",
       "   18480,\n",
       "   13,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643],\n",
       "  [50,\n",
       "   5178,\n",
       "   13,\n",
       "   91929,\n",
       "   477,\n",
       "   22012,\n",
       "   74964,\n",
       "   11,\n",
       "   220,\n",
       "   18,\n",
       "   18,\n",
       "   11,\n",
       "   572,\n",
       "   5001,\n",
       "   2114,\n",
       "   504,\n",
       "   6216,\n",
       "   36306,\n",
       "   311,\n",
       "   55344,\n",
       "   806,\n",
       "   6554,\n",
       "   304,\n",
       "   42949,\n",
       "   18480,\n",
       "   13,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643],\n",
       "  [50,\n",
       "   5178,\n",
       "   13,\n",
       "   91929,\n",
       "   477,\n",
       "   22012,\n",
       "   74964,\n",
       "   11,\n",
       "   220,\n",
       "   18,\n",
       "   18,\n",
       "   11,\n",
       "   572,\n",
       "   5001,\n",
       "   2114,\n",
       "   504,\n",
       "   6216,\n",
       "   36306,\n",
       "   311,\n",
       "   55344,\n",
       "   806,\n",
       "   6554,\n",
       "   304,\n",
       "   42949,\n",
       "   18480,\n",
       "   13,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643],\n",
       "  [50,\n",
       "   5178,\n",
       "   13,\n",
       "   91929,\n",
       "   477,\n",
       "   22012,\n",
       "   74964,\n",
       "   11,\n",
       "   220,\n",
       "   18,\n",
       "   18,\n",
       "   11,\n",
       "   572,\n",
       "   5001,\n",
       "   2114,\n",
       "   504,\n",
       "   6216,\n",
       "   36306,\n",
       "   311,\n",
       "   55344,\n",
       "   806,\n",
       "   6554,\n",
       "   304,\n",
       "   42949,\n",
       "   18480,\n",
       "   13,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643]],\n",
       " 'attention_mask': [[1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0]],\n",
       " 'input_ids_2': [[2121,\n",
       "   358,\n",
       "   46080,\n",
       "   1526,\n",
       "   1045,\n",
       "   59886,\n",
       "   2213,\n",
       "   11,\n",
       "   358,\n",
       "   49057,\n",
       "   5193,\n",
       "   264,\n",
       "   67132,\n",
       "   9487,\n",
       "   429,\n",
       "   11660,\n",
       "   311,\n",
       "   16400,\n",
       "   1246,\n",
       "   697,\n",
       "   7194,\n",
       "   1841,\n",
       "   1410,\n",
       "   2990,\n",
       "   498,\n",
       "   323,\n",
       "   697,\n",
       "   8263,\n",
       "   311,\n",
       "   279,\n",
       "   1429,\n",
       "   151643],\n",
       "  [2132,\n",
       "   374,\n",
       "   47596,\n",
       "   429,\n",
       "   358,\n",
       "   49360,\n",
       "   847,\n",
       "   18894,\n",
       "   46191,\n",
       "   2645,\n",
       "   8826,\n",
       "   419,\n",
       "   4925,\n",
       "   11,\n",
       "   438,\n",
       "   1741,\n",
       "   8679,\n",
       "   35741,\n",
       "   81729,\n",
       "   21975,\n",
       "   11,\n",
       "   892,\n",
       "   304,\n",
       "   2484,\n",
       "   3059,\n",
       "   304,\n",
       "   79053,\n",
       "   24473,\n",
       "   2176,\n",
       "   50946,\n",
       "   398,\n",
       "   151643],\n",
       "  [785,\n",
       "   3139,\n",
       "   315,\n",
       "   91272,\n",
       "   2494,\n",
       "   11514,\n",
       "   3545,\n",
       "   11508,\n",
       "   311,\n",
       "   5089,\n",
       "   25709,\n",
       "   323,\n",
       "   8660,\n",
       "   13,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643],\n",
       "  [2121,\n",
       "   1340,\n",
       "   12290,\n",
       "   448,\n",
       "   279,\n",
       "   18960,\n",
       "   11,\n",
       "   264,\n",
       "   12060,\n",
       "   315,\n",
       "   78057,\n",
       "   37493,\n",
       "   916,\n",
       "   1059,\n",
       "   11,\n",
       "   3602,\n",
       "   1340,\n",
       "   5644,\n",
       "   8630,\n",
       "   279,\n",
       "   3900,\n",
       "   429,\n",
       "   279,\n",
       "   56783,\n",
       "   748,\n",
       "   11618,\n",
       "   311,\n",
       "   26549,\n",
       "   1035,\n",
       "   10354,\n",
       "   2041,\n",
       "   151643],\n",
       "  [2121,\n",
       "   358,\n",
       "   3736,\n",
       "   498,\n",
       "   3063,\n",
       "   11,\n",
       "   847,\n",
       "   4746,\n",
       "   2021,\n",
       "   6436,\n",
       "   448,\n",
       "   21770,\n",
       "   518,\n",
       "   1660,\n",
       "   697,\n",
       "   56164,\n",
       "   11,\n",
       "   22216,\n",
       "   10976,\n",
       "   1449,\n",
       "   4445,\n",
       "   13,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643],\n",
       "  [56689,\n",
       "   2114,\n",
       "   311,\n",
       "   2182,\n",
       "   806,\n",
       "   6554,\n",
       "   311,\n",
       "   2732,\n",
       "   304,\n",
       "   42949,\n",
       "   18480,\n",
       "   11,\n",
       "   274,\n",
       "   5178,\n",
       "   13,\n",
       "   91929,\n",
       "   477,\n",
       "   22012,\n",
       "   74964,\n",
       "   3697,\n",
       "   2114,\n",
       "   504,\n",
       "   6216,\n",
       "   36306,\n",
       "   13,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643],\n",
       "  [56689,\n",
       "   2114,\n",
       "   311,\n",
       "   2182,\n",
       "   806,\n",
       "   6554,\n",
       "   311,\n",
       "   2732,\n",
       "   304,\n",
       "   42949,\n",
       "   18480,\n",
       "   11,\n",
       "   274,\n",
       "   5178,\n",
       "   13,\n",
       "   91929,\n",
       "   477,\n",
       "   22012,\n",
       "   74964,\n",
       "   5927,\n",
       "   504,\n",
       "   6216,\n",
       "   36306,\n",
       "   13,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643],\n",
       "  [1249,\n",
       "   2182,\n",
       "   806,\n",
       "   6554,\n",
       "   311,\n",
       "   2732,\n",
       "   304,\n",
       "   42949,\n",
       "   18480,\n",
       "   11,\n",
       "   274,\n",
       "   5178,\n",
       "   13,\n",
       "   91929,\n",
       "   477,\n",
       "   22012,\n",
       "   74964,\n",
       "   5927,\n",
       "   504,\n",
       "   6216,\n",
       "   36306,\n",
       "   13,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643],\n",
       "  [97904,\n",
       "   806,\n",
       "   6554,\n",
       "   311,\n",
       "   2732,\n",
       "   304,\n",
       "   42949,\n",
       "   18480,\n",
       "   11,\n",
       "   274,\n",
       "   5178,\n",
       "   13,\n",
       "   91929,\n",
       "   477,\n",
       "   22012,\n",
       "   74964,\n",
       "   5927,\n",
       "   504,\n",
       "   6216,\n",
       "   36306,\n",
       "   13,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643],\n",
       "  [97904,\n",
       "   806,\n",
       "   6554,\n",
       "   311,\n",
       "   2732,\n",
       "   304,\n",
       "   42949,\n",
       "   18480,\n",
       "   11,\n",
       "   274,\n",
       "   5178,\n",
       "   13,\n",
       "   91929,\n",
       "   477,\n",
       "   22012,\n",
       "   74964,\n",
       "   5927,\n",
       "   504,\n",
       "   6216,\n",
       "   36306,\n",
       "   518,\n",
       "   4231,\n",
       "   220,\n",
       "   18,\n",
       "   18,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643]],\n",
       " 'attention_mask_2': [[1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0]],\n",
       " 'input_ids_3': [[40,\n",
       "   2666,\n",
       "   90074,\n",
       "   2167,\n",
       "   358,\n",
       "   2666,\n",
       "   3093,\n",
       "   315,\n",
       "   1077,\n",
       "   1495,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643],\n",
       "  [95414,\n",
       "   11,\n",
       "   358,\n",
       "   2666,\n",
       "   264,\n",
       "   2699,\n",
       "   48130,\n",
       "   9120,\n",
       "   847,\n",
       "   12923,\n",
       "   773,\n",
       "   3265,\n",
       "   448,\n",
       "   1059,\n",
       "   501,\n",
       "   4780,\n",
       "   1393,\n",
       "   358,\n",
       "   4249,\n",
       "   1588,\n",
       "   7484,\n",
       "   13,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643],\n",
       "  [40,\n",
       "   2058,\n",
       "   1366,\n",
       "   311,\n",
       "   714,\n",
       "   358,\n",
       "   2666,\n",
       "   1075,\n",
       "   358,\n",
       "   56030,\n",
       "   323,\n",
       "   83871,\n",
       "   498,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643],\n",
       "  [40,\n",
       "   2677,\n",
       "   2666,\n",
       "   25195,\n",
       "   369,\n",
       "   279,\n",
       "   15888,\n",
       "   501,\n",
       "   1251,\n",
       "   4446,\n",
       "   11,\n",
       "   714,\n",
       "   438,\n",
       "   5135,\n",
       "   438,\n",
       "   358,\n",
       "   19131,\n",
       "   1105,\n",
       "   311,\n",
       "   847,\n",
       "   1850,\n",
       "   4238,\n",
       "   11,\n",
       "   429,\n",
       "   23009,\n",
       "   4977,\n",
       "   311,\n",
       "   21134,\n",
       "   1526,\n",
       "   847,\n",
       "   19225,\n",
       "   151643],\n",
       "  [40,\n",
       "   646,\n",
       "   1405,\n",
       "   1492,\n",
       "   714,\n",
       "   1744,\n",
       "   419,\n",
       "   1035,\n",
       "   614,\n",
       "   1012,\n",
       "   26200,\n",
       "   438,\n",
       "   1616,\n",
       "   2238,\n",
       "   27759,\n",
       "   369,\n",
       "   5883,\n",
       "   1182,\n",
       "   304,\n",
       "   279,\n",
       "   1899,\n",
       "   13,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643,\n",
       "   151643],\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None],\n",
       " 'attention_mask_3': [[1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None],\n",
       " 'active_heads': ['emotion',\n",
       "  'emotion',\n",
       "  'emotion',\n",
       "  'emotion',\n",
       "  'emotion',\n",
       "  'apt_label',\n",
       "  'apt_label',\n",
       "  'apt_label',\n",
       "  'apt_label',\n",
       "  'apt_label'],\n",
       " 'labels': [0, 5, 3, 1, 0, 0, 0, 0, 0, 1]}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_datasets[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorForBiEncoder(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=\"max_length\",\n",
    "    pad_to_multiple_of=None\n",
    ")\n",
    "\n",
    "data = data_collator(train_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False, False, False, False, False, True, True, True, True, True]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data[\"active_heads\"]のうち、\"apt_label\"をもつかどうかのバイナリリスト\n",
    "head_idx = [True if head == \"apt_label\" else False for head in data[\"active_heads\"]]\n",
    "head_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'apt_label', 1: 'emotion'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2head = {i: head for i, head in enumerate(clf_configs.keys())}\n",
    "id2head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtype: torch.bfloat16, device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/remote/csifs1/disk3/users/yama11235/yama11235/SLBERT/utils/sts2/clf_trainer.py:83: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from sts2.clf_trainer import CustomTrainer\n",
    "from functools import partial\n",
    "from sts2.metrics import compute_metrics\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"  # CUDAの同期を有効にしてデバッグしやすくする\n",
    "\n",
    "compute_fn = partial(\n",
    "    compute_metrics,\n",
    "    classifier_configs=clf_configs,\n",
    "    id2_head=id2head,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForBiEncoder(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=\"max_length\",\n",
    "    pad_to_multiple_of=None\n",
    ")\n",
    "\n",
    "corr_labels = {\"apt_label\": {\"emotion\": 0.1}}\n",
    "# corr_labels = {\n",
    "#   \"argpairs_score\": {\"bws_score\": 0.6}\n",
    "# }\n",
    "\n",
    "# CUDAをリセット\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "from sts2.modeling_encoders import BiEncoderForClassification\n",
    "model = BiEncoderForClassification(model_config=config, classifier_configs=clf_configs)\n",
    "# 全パラメータを凍結\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in model.backbone.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=HFTrainingArguments(\n",
    "        output_dir=\"./output_test\",\n",
    "        overwrite_output_dir=True,\n",
    "        do_train=True,\n",
    "        do_eval=True,\n",
    "        eval_strategy=\"epoch\",\n",
    "        logging_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_dir=\"./logs\",\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=32,\n",
    "        num_train_epochs=1,\n",
    "        weight_decay=0.01,\n",
    "        seed=42,\n",
    "        learning_rate=1e-6,\n",
    "        remove_unused_columns=False,\n",
    "        report_to=\"wandb\",  # wandbを使用する場合はここを設定\n",
    "    ),\n",
    "    classifier_configs=clf_configs,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_datasets,\n",
    "    eval_dataset=train_datasets,  # ここは適切な検証データセットに置き換えてください\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_fn,\n",
    "    corr_labels=corr_labels,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataloader = trainer.get_eval_dataloader()\n",
    "\n",
    "# 最初のバッチだけ取り出して確認\n",
    "batch = next(iter(eval_dataloader))\n",
    "# batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds: {'apt_label': array([      nan,       nan,       nan,       nan,       nan, 0.7265625,\n",
      "       0.7265625, 0.7265625, 0.7265625, 0.71875  ], dtype=float32), 'emotion': array([       nan,        nan,        nan,        nan,        nan,\n",
      "       0.97265625, 0.9765625 , 0.96875   , 0.9765625 , 0.9453125 ],\n",
      "      dtype=float32), 'overall_similarity': array([       nan,        nan,        nan,        nan,        nan,\n",
      "       0.94921875, 0.94140625, 0.93359375, 0.9375    , 0.9375    ],\n",
      "      dtype=float32), 'apt_label_pos_similarity': array([0.90625   , 0.9296875 , 0.8671875 , 0.94140625, 0.8984375 ,\n",
      "              nan,        nan,        nan,        nan,        nan],\n",
      "      dtype=float32), 'apt_label_neg_similarity': array([0.91015625, 0.91796875, 0.890625  , 0.9453125 , 0.92578125,\n",
      "              nan,        nan,        nan,        nan,        nan],\n",
      "      dtype=float32), 'emotion_pos_similarity': array([0.89453125, 0.9375    , 0.875     , 0.94921875, 0.921875  ,\n",
      "              nan,        nan,        nan,        nan,        nan],\n",
      "      dtype=float32), 'emotion_neg_similarity': array([0.92578125, 0.9375    , 0.890625  , 0.92578125, 0.94921875,\n",
      "              nan,        nan,        nan,        nan,        nan],\n",
      "      dtype=float32), 'emotion_anchor_prob': array([[0.09375   , 0.24707031, 0.22753906, 0.14257812, 0.05957031,\n",
      "        0.23046875],\n",
      "       [0.11035156, 0.20507812, 0.2109375 , 0.17382812, 0.07421875,\n",
      "        0.22558594],\n",
      "       [0.11425781, 0.22460938, 0.22558594, 0.16015625, 0.07617188,\n",
      "        0.19824219],\n",
      "       [0.11181641, 0.20605469, 0.20800781, 0.203125  , 0.07617188,\n",
      "        0.19433594],\n",
      "       [0.10498047, 0.24609375, 0.22851562, 0.13769531, 0.08740234,\n",
      "        0.19628906],\n",
      "       [       nan,        nan,        nan,        nan,        nan,\n",
      "               nan],\n",
      "       [       nan,        nan,        nan,        nan,        nan,\n",
      "               nan],\n",
      "       [       nan,        nan,        nan,        nan,        nan,\n",
      "               nan],\n",
      "       [       nan,        nan,        nan,        nan,        nan,\n",
      "               nan],\n",
      "       [       nan,        nan,        nan,        nan,        nan,\n",
      "               nan]], dtype=float32), 'emotion_positive_prob': array([[0.1328125 , 0.23730469, 0.19335938, 0.15527344, 0.08105469,\n",
      "        0.20117188],\n",
      "       [0.11865234, 0.23828125, 0.21972656, 0.15039062, 0.07324219,\n",
      "        0.19921875],\n",
      "       [0.12158203, 0.27929688, 0.19042969, 0.1328125 , 0.08154297,\n",
      "        0.19335938],\n",
      "       [0.09619141, 0.2109375 , 0.23046875, 0.19433594, 0.06445312,\n",
      "        0.20410156],\n",
      "       [0.10839844, 0.20019531, 0.24511719, 0.171875  , 0.07666016,\n",
      "        0.19824219],\n",
      "       [       nan,        nan,        nan,        nan,        nan,\n",
      "               nan],\n",
      "       [       nan,        nan,        nan,        nan,        nan,\n",
      "               nan],\n",
      "       [       nan,        nan,        nan,        nan,        nan,\n",
      "               nan],\n",
      "       [       nan,        nan,        nan,        nan,        nan,\n",
      "               nan],\n",
      "       [       nan,        nan,        nan,        nan,        nan,\n",
      "               nan]], dtype=float32), 'emotion_negative_prob': array([[0.11181641, 0.265625  , 0.20703125, 0.14453125, 0.07958984,\n",
      "        0.19042969],\n",
      "       [0.09912109, 0.20019531, 0.24804688, 0.17089844, 0.07861328,\n",
      "        0.20214844],\n",
      "       [0.11425781, 0.25195312, 0.21386719, 0.14746094, 0.08642578,\n",
      "        0.18652344],\n",
      "       [0.11376953, 0.24316406, 0.20410156, 0.14160156, 0.08447266,\n",
      "        0.21289062],\n",
      "       [0.09863281, 0.23535156, 0.22265625, 0.15722656, 0.06835938,\n",
      "        0.21777344],\n",
      "       [       nan,        nan,        nan,        nan,        nan,\n",
      "               nan],\n",
      "       [       nan,        nan,        nan,        nan,        nan,\n",
      "               nan],\n",
      "       [       nan,        nan,        nan,        nan,        nan,\n",
      "               nan],\n",
      "       [       nan,        nan,        nan,        nan,        nan,\n",
      "               nan],\n",
      "       [       nan,        nan,        nan,        nan,        nan,\n",
      "               nan]], dtype=float32), 'overall_pos_similarity': array([0.578125  , 0.62890625, 0.55859375, 0.51171875, 0.5078125 ,\n",
      "              nan,        nan,        nan,        nan,        nan],\n",
      "      dtype=float32), 'overall_neg_similarity': array([0.7109375 , 0.51171875, 0.49023438, 0.59765625, 0.51953125,\n",
      "              nan,        nan,        nan,        nan,        nan],\n",
      "      dtype=float32)}\n",
      "labels: [0. 5. 3. 1. 0. 0. 0. 0. 0. 1.]\n",
      "active_heads: [1 1 1 1 1 0 0 0 0 0]\n",
      "Head: apt_label, vector length: 15\n",
      "Head: emotion, vector length: 15\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 4.407558441162109,\n",
       " 'eval_model_preparation_time': 0.004,\n",
       " 'eval_apt_label_best-threshold': 0.0,\n",
       " 'eval_apt_label_best-accuracy': 0.2,\n",
       " 'eval_apt_label_best-f1': 0.3333333333333333,\n",
       " 'eval_apt_label_auc': 0.0,\n",
       " 'eval_apt_label_mse': 0.438134765625,\n",
       " 'eval_emotion_triplet_accuracy': 0.2,\n",
       " 'eval_emotion_avg_positive_similarity': 0.915625,\n",
       " 'eval_emotion_avg_negative_similarity': 0.92578125,\n",
       " 'eval_emotion_anchor_accuracy': 0.2,\n",
       " 'eval_emotion_anchor_macro_f1': 0.2,\n",
       " 'eval_apt_label_vs_emotion_pearson': -0.5973093854732312,\n",
       " 'eval_apt_label_vs_emotion_spearman': -0.3345438017134364,\n",
       " 'eval_runtime': 0.249,\n",
       " 'eval_samples_per_second': 40.156,\n",
       " 'eval_steps_per_second': 4.016}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(eval_dataset=train_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing correlation: argpairs_score vs bws_score, 4, 4, corr: 0.9342879056930542, target: 0.6000000238418579\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8/8 00:01, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Argpairs Score Mse</th>\n",
       "      <th>Argpairs Score Pearson</th>\n",
       "      <th>Bws Score Mse</th>\n",
       "      <th>Bws Score Pearson</th>\n",
       "      <th>Argpairs Score Vs Bws Score Pearson</th>\n",
       "      <th>Argpairs Score Vs Bws Score Spearman</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.595400</td>\n",
       "      <td>0.103479</td>\n",
       "      <td>0.336996</td>\n",
       "      <td>0.482103</td>\n",
       "      <td>0.207029</td>\n",
       "      <td>0.547772</td>\n",
       "      <td>0.921682</td>\n",
       "      <td>0.787035</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing correlation: argpairs_score vs bws_score, 4, 4, corr: -0.004522493574768305, target: 0.6000000238418579\n",
      "Processing correlation: argpairs_score vs bws_score, 4, 4, corr: -0.09636474400758743, target: 0.6000000238418579\n",
      "Processing correlation: argpairs_score vs bws_score, 4, 4, corr: 0.8299806714057922, target: 0.6000000238418579\n",
      "Processing correlation: argpairs_score vs bws_score, 4, 4, corr: -0.3418736457824707, target: 0.6000000238418579\n",
      "Processing correlation: argpairs_score vs bws_score, 4, 4, corr: -0.5798406600952148, target: 0.6000000238418579\n",
      "Processing correlation: argpairs_score vs bws_score, 4, 4, corr: -0.5565418601036072, target: 0.6000000238418579\n",
      "Processing correlation: argpairs_score vs bws_score, 4, 4, corr: 0.962778627872467, target: 0.6000000238418579\n",
      "Processing correlation: argpairs_score vs bws_score, 32, 32, corr: 0.9216816425323486, target: 0.6000000238418579\n",
      "Head: argpairs_score, vector length: 32\n",
      "Head: bws_score, vector length: 32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=8, training_loss=0.5954198241233826, metrics={'train_runtime': 1.7263, 'train_samples_per_second': 18.536, 'train_steps_per_second': 4.634, 'total_flos': 43351302733824.0, 'train_loss': 0.5954198241233826, 'epoch': 1.0})"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 途中から"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = ['ArgPairs']\n",
    "\n",
    "classifier_list = [\"argpairs_score\"]\n",
    "\n",
    "path = \"output/mixedbread-ai/mxbai-embed-large-v1\"\n",
    "# path = \"output/Qwen/Qwen3-Embedding-8B\"\n",
    "layer_num = 23\n",
    "# layer_num = 35\n",
    "configs = {}\n",
    "clf_path = []\n",
    "for train_name, classifier_path in zip(train_list, classifier_list):\n",
    "    with open(os.path.join(path, train_name, f\"lr:1e-4_seed:42_layer:{layer_num}\", f\"{train_name}.json\")) as f:\n",
    "        data = json.load(f)\n",
    "        configs[classifier_path] = list(data.values())[0]\n",
    "        clf_path.append(os.path.join(path, train_name, f\"lr:1e-4_seed:42_layer:{layer_num}\", \n",
    "        f\"{list(data.values())[0]['type']}_layer:{list(data.values())[0]['layer']}_dim:{list(data.values())[0]['output_dim']}\",\n",
    "        f\"{classifier_path}_classifier.bin\")\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['output/mixedbread-ai/mxbai-embed-large-v1/ArgPairs/lr:1e-4_seed:42_layer:23/linear_layer:23_dim:256/argpairs_score_classifier.bin']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"architectures\": [\n",
       "    \"BertModel\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"attn_implementation\": \"eager\",\n",
       "  \"cache_dir\": null,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"classifier_save_directory\": \"./output_test\",\n",
       "  \"config_name\": null,\n",
       "  \"device_map\": \"cuda\",\n",
       "  \"encoding_type\": \"bi_encoder\",\n",
       "  \"freeze_encoder\": true,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 1024,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 4096,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_name_or_path\": \"mixedbread-ai/mxbai-embed-large-v1\",\n",
       "  \"model_revision\": \"main\",\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 16,\n",
       "  \"num_hidden_layers\": 24,\n",
       "  \"objective\": \"mse\",\n",
       "  \"pad_token_id\": 0,\n",
       "  \"pooler_type\": \"avg\",\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"tokenizer_name\": null,\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transform\": false,\n",
       "  \"transformers_version\": \"4.53.0\",\n",
       "  \"triencoder_head\": \"hadamard\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_auth_token\": null,\n",
       "  \"use_cache\": false,\n",
       "  \"use_fast_tokenizer\": true,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"mixedbread-ai/mxbai-embed-large-v1\"\n",
    "# model_path = \"/works/data3/users/yama11235/yama11235/model/hoo-cache/huggingface/hub/models--mixedbread-ai--mxbai-embed-large-v1/snapshots/db9d1fe0f31addb4978201b2bf3e577f3f8900d2\"\n",
    "config = AutoConfig.from_pretrained(model_path)\n",
    "config.model_name_or_path = model_path  # ここを追加\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "config.attn_implementation = \"eager\" # sdpaのエラー対策\n",
    "# config.device_map = \"auto\"\n",
    "config.device_map = \"cuda\"\n",
    "# config.device_map = None\n",
    "config.config_name = None\n",
    "config.tokenizer_name = None\n",
    "config.cache_dir = None\n",
    "config.use_fast_tokenizer = True\n",
    "config.model_revision = \"main\"\n",
    "config.use_auth_token = None\n",
    "# config.torch_dtype= \"bfloat16\"  # データ型をbfloat16に設定\n",
    "config.torch_dtype = torch.float32  # データ型をfloat32に設定\n",
    "# config.torch_dtype = torch.float16  # データ型をfloat16に設定\n",
    "# config.attn_implementation = \"flash_attention_2\"  \n",
    "config.objective = \"mse\"\n",
    "config.encoding_type = \"bi_encoder\"\n",
    "# config.pooler_type = \"last\"\n",
    "config.pooler_type = \"avg\"\n",
    "config.freeze_encoder = True\n",
    "config.transform = False\n",
    "config.triencoder_head = \"hadamard\"\n",
    "config.classifier_save_directory = \"./output_test\"\n",
    "\n",
    "model_config = config\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtype: torch.float32, device: cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BiEncoderForClassification(\n",
       "  (backbone): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 1024, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 1024)\n",
       "      (token_type_embeddings): Embedding(2, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (pooler): Pooler()\n",
       "  (embedding_classifiers): ModuleDict(\n",
       "    (argpairs_score): LinearLayer(\n",
       "      (linear): Sequential(\n",
       "        (0): Dropout(p=0.1, inplace=False)\n",
       "        (1): Linear(in_features=1024, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BiEncoderForClassification.from_pretrained(\n",
    "    model_path,\n",
    "    model_config,\n",
    "    clf_path,\n",
    "    configs,\n",
    "    # classifier_freeze=[]\n",
    "    )\n",
    "model.eval()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "babb3c3ade57467482dd0300ee384322",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# raw_datasets1 = load_dataset(\"csv\", data_files=\"/works/data3/users/yama11235/yama11235/SLBERT/utils/data_style/ELSA_joy-sadness-anger-surprise-love-fear_test.csv\")\n",
    "# raw_datasets2 = load_dataset(\"csv\", data_files=\"data/APT_train.csv\")\n",
    "# raw_datasets = datasets.concatenate_datasets([raw_datasets1[\"train\"].select(range(5)), raw_datasets2[\"train\"].select(range(5))])\n",
    "raw_datasets1 = load_dataset(\"csv\", data_files=\"data_preprocessed/ArgPairs_test.csv\")\n",
    "# raw_datasets2 = load_dataset(\"csv\", data_files=\"/works/data3/users/yama11235/yama11235/SLBERT/utils/data_preprocessed/BWS_test.csv\")\n",
    "# raw_datasets = datasets.concatenate_datasets([raw_datasets1[\"train\"].select(range(16)), raw_datasets2[\"train\"].select(range(16))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_function = get_preprocessing_function(\n",
    "    tokenizer=tokenizer,\n",
    "    sentence1_key=\"sentence1\",\n",
    "    sentence2_key=\"sentence2\",\n",
    "    sentence3_key=\"sentence3\",\n",
    "    sentence3_flag=False,\n",
    "    # aspect_key=['emotion', 'apt_label'],\n",
    "    # aspect_key=['argpairs_score', 'bws_score'],\n",
    "    aspect_key=['argpairs_score'],\n",
    "    padding=\"max_length\",\n",
    "    max_seq_length=512,\n",
    "    model_args=config,\n",
    "    scale=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf785fca22134c4585e8f0ac814669c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_datasets = raw_datasets1.map(\n",
    "    preprocessing_function,\n",
    "    batched=False,\n",
    "    remove_columns=raw_datasets1[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForBiEncoder(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=\"max_length\",\n",
    "    pad_to_multiple_of=None\n",
    ")\n",
    "\n",
    "# data = data_collator(train_datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datasets = train_datasets[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'argpairs_score'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2head = {i: head for i, head in enumerate(configs.keys())}\n",
    "id2head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/remote/csifs1/disk3/users/yama11235/yama11235/SLBERT/utils/sts2/clf_trainer.py:83: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from sts2.clf_trainer import CustomTrainer\n",
    "from functools import partial\n",
    "from sts2.metrics import compute_metrics\n",
    "\n",
    "compute_fn = partial(\n",
    "    compute_metrics,\n",
    "    classifier_configs=configs,\n",
    "    id2_head=id2head,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForBiEncoder(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=\"max_length\",\n",
    "    pad_to_multiple_of=None\n",
    ")\n",
    "\n",
    "# corr_labels = {\"apt_label\": {\"emotion\": 0.1}}\n",
    "corr_labels = {\n",
    "  \"argpairs_score\": {\"bws_score\": 0.6}\n",
    "}\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=HFTrainingArguments(\n",
    "        output_dir=\"./output_test\",\n",
    "        overwrite_output_dir=True,\n",
    "        do_train=True,\n",
    "        do_eval=True,\n",
    "        eval_strategy=\"epoch\",\n",
    "        logging_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_dir=\"./logs\",\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=64,\n",
    "        num_train_epochs=1,\n",
    "        weight_decay=0.01,\n",
    "        seed=42,\n",
    "        learning_rate=1e-6,\n",
    "        remove_unused_columns=False,\n",
    "        report_to=\"wandb\",  # wandbを使用する場合はここを設定\n",
    "    ),\n",
    "    classifier_configs=configs,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_datasets,\n",
    "    eval_dataset=train_datasets,  # ここは適切な検証データセットに置き換えてください\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_fn,\n",
    "    corr_labels=corr_labels,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19/19 00:25]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions keys: dict_keys(['argpairs_score', 'overall_similarity', 'argpairs_score_pos_similarity', 'argpairs_score_neg_similarity', 'overall_pos_similarity', 'overall_neg_similarity'])\n",
      "label_dict keys: dict_keys(['labels', 'active_heads'])\n",
      "predictions shape: 6, label_dict shape: 2\n",
      "preds: {'argpairs_score': array([0.23437788, 0.80355227, 0.70648766, ..., 0.27411968, 0.6079304 ,\n",
      "       0.6051959 ], shape=(1200,), dtype=float32), 'overall_similarity': array([0.7324028 , 0.8853478 , 0.8460119 , ..., 0.78702384, 0.8311864 ,\n",
      "       0.74303734], shape=(1200,), dtype=float32), 'argpairs_score_pos_similarity': array([nan, nan, nan, ..., nan, nan, nan], shape=(1200,), dtype=float32), 'argpairs_score_neg_similarity': array([nan, nan, nan, ..., nan, nan, nan], shape=(1200,), dtype=float32), 'overall_pos_similarity': array([nan, nan, nan, ..., nan, nan, nan], shape=(1200,), dtype=float32), 'overall_neg_similarity': array([nan, nan, nan, ..., nan, nan, nan], shape=(1200,), dtype=float32)}\n",
      "labels: [[0.42857143]\n",
      " [1.        ]\n",
      " [0.78571427]\n",
      " ...\n",
      " [0.14285715]\n",
      " [0.14285715]\n",
      " [0.5       ]]\n",
      "active_heads: [0 0 0 ... 0 0 0]\n",
      "Head: argpairs_score, scores shape: (1200,), truths shape: (1200,)\n",
      "Head: argpairs_score, vector length: 1200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m2959648335\u001b[0m (\u001b[33m2959648335-university-of-tokyo\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/remote/csifs1/disk3/users/yama11235/yama11235/SLBERT/utils/wandb/run-20250715_004727-4exyiitk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/2959648335-university-of-tokyo/huggingface/runs/4exyiitk' target=\"_blank\">./output_test</a></strong> to <a href='https://wandb.ai/2959648335-university-of-tokyo/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/2959648335-university-of-tokyo/huggingface' target=\"_blank\">https://wandb.ai/2959648335-university-of-tokyo/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/2959648335-university-of-tokyo/huggingface/runs/4exyiitk' target=\"_blank\">https://wandb.ai/2959648335-university-of-tokyo/huggingface/runs/4exyiitk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.030230019241571426,\n",
       " 'eval_model_preparation_time': 0.0029,\n",
       " 'eval_argpairs_score_mse': 0.030230020552310773,\n",
       " 'eval_argpairs_score_pearson': 0.7272617397508163,\n",
       " 'eval_argpairs_score_spearman': 0.7212626764282644,\n",
       " 'eval_runtime': 27.5583,\n",
       " 'eval_samples_per_second': 43.544,\n",
       " 'eval_steps_per_second': 0.689}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(eval_dataset=train_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence3', 'input_ids', 'token_type_ids', 'attention_mask', 'input_ids_2', 'attention_mask_2', 'token_type_ids_2', 'active_heads', 'labels'],\n",
       "    num_rows: 1200\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_tensor = torch.tensor([], dtype=torch.float32)\n",
    "labels_tensor = torch.tensor([], dtype=torch.float32)\n",
    "with torch.no_grad():\n",
    "    eval_dataloader = trainer.get_eval_dataloader()\n",
    "    for batch in eval_dataloader:\n",
    "        outputs = model(\n",
    "            input_ids=batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "            token_type_ids=None,\n",
    "            input_ids_2=batch[\"input_ids_2\"],\n",
    "            attention_mask_2=batch[\"attention_mask_2\"],\n",
    "            token_type_ids_2=None,\n",
    "            input_ids_3=None,\n",
    "            attention_mask_3=None,\n",
    "            token_type_ids_3=None,\n",
    "        )\n",
    "        output_tensor = torch.cat((output_tensor.cpu(), outputs['argpairs_score'].detach().clone().cpu()), dim=0)\n",
    "        labels_tensor = torch.cat((labels_tensor.cpu(), batch[\"labels\"].flatten().detach().clone().cpu()), dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PearsonRResult(statistic=0.68120724, pvalue=1.895030045385178e-164)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearson_corr = pearsonr(output_tensor.numpy(), labels_tensor.numpy())\n",
    "pearson_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SignificanceResult(statistic=0.6782949622952499, pvalue=1.5636668291218919e-162)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spearman_corr = spearmanr(output_tensor.numpy(), labels_tensor.numpy())\n",
    "spearman_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1_list = []\n",
    "sent2_list = []\n",
    "labels_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    eval_dataloader = trainer.get_eval_dataloader()\n",
    "    for batch in eval_dataloader:\n",
    "        sent1 = model.encode(\n",
    "            input_ids=batch[\"input_ids\"].to(model.device),\n",
    "            attention_mask=batch[\"attention_mask\"].to(model.device),\n",
    "            token_type_ids=None,\n",
    "        )\n",
    "        sent1_list.append(sent1[\"argpairs_score\"].cpu().numpy())\n",
    "        sent2 = model.encode(\n",
    "            input_ids=batch[\"input_ids_2\"].to(model.device),\n",
    "            attention_mask=batch[\"attention_mask_2\"].to(model.device),\n",
    "            token_type_ids=None,\n",
    "        )\n",
    "        sent2_list.append(sent2[\"argpairs_score\"].cpu().numpy())\n",
    "        labels_list.append(batch[\"labels\"].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00360563,  0.25461003,  0.20472933, ..., -0.40114874,\n",
       "         0.57263595, -0.09370866],\n",
       "       [-0.14023702, -0.10293227,  0.61865985, ..., -0.24134655,\n",
       "        -0.3402528 ,  0.38966277],\n",
       "       [ 0.05375183,  0.59631807,  0.42281276, ..., -0.13983706,\n",
       "         0.2536421 ,  0.12120374],\n",
       "       ...,\n",
       "       [-0.20158575, -0.1763052 ,  0.32370704, ..., -0.14507358,\n",
       "        -0.41976887, -0.3233777 ],\n",
       "       [-0.36450148, -0.00383214,  0.45837742, ..., -0.25293654,\n",
       "        -0.05994258,  0.1226934 ],\n",
       "       [-0.35413396, -0.09773616,  0.56367236, ..., -0.31176034,\n",
       "         0.2788598 ,  0.45976478]], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent1_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sent1_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened1 = np.concatenate(sent1_list, axis=0)\n",
    "flattened2 = np.concatenate(sent2_list, axis=0)\n",
    "labels = np.concatenate(labels_list, axis=0).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_list = []\n",
    "for i in range(len(flattened1)):\n",
    "    similarity = torch.cosine_similarity(\n",
    "        torch.tensor(flattened1[i]), \n",
    "        torch.tensor(flattened2[i]), \n",
    "        dim=0\n",
    "    ).item()\n",
    "    similarity_list.append(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PearsonRResult(statistic=0.7272617260896164, pvalue=4.898315025461355e-198)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr(similarity_list, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SignificanceResult(statistic=0.7212626764282644, pvalue=2.808785295498658e-193)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spearmanr(similarity_list, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "model.to(\"cuda\")\n",
    "# モデル全体のパラメータが GPU 上か\n",
    "print(next(model.parameters()).device)  \n",
    "# 分類ヘッドの重みが GPU 上か\n",
    "print(model.embedding_classifiers['argpairs_score'].linear[1].weight.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiEncoderForClassification(\n",
       "  (backbone): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 1024, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 1024)\n",
       "      (token_type_embeddings): Embedding(2, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (pooler): Pooler()\n",
       "  (embedding_classifiers): ModuleDict(\n",
       "    (argpairs_score): LinearLayer(\n",
       "      (linear): Sequential(\n",
       "        (0): Dropout(p=0.1, inplace=False)\n",
       "        (1): Linear(in_features=1024, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
