{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# train.py フロー検証 (nGPT classifier)\n",
        "\n",
        "`Sentiment-Circle/utils/demo2.ipynb` と同じ方針で、`train.py` の実行ステップを 1 つずつ追跡しながら **nGPT 分類器** をテストできるようにした検証ノートブックです。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. 準備\n",
        "- `train.py` で定義されているデータセット前処理・トレーナー初期化の関数を直接呼び出し、フローをそのまま再現します。\n",
        "- Weights & Biases 連携はデバッグ用途なので無効化しています (`WANDB_MODE=disabled`)。\n",
        "- `Train_df.csv` / `Valid_df.csv` / `Test_df.csv` から少数サンプルを取り、計算負荷を抑えます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "8d672e62",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Project root: /remote/csifs1/disk3/users/yama11235/yama11235/Sentiment-Circle\n",
            "CUDA available: True\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "import pathlib\n",
        "import random\n",
        "import sys\n",
        "from functools import partial\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoConfig, AutoTokenizer, PrinterCallback\n",
        "\n",
        "os.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\n",
        "os.environ.setdefault(\"WANDB_MODE\", \"disabled\")\n",
        "os.environ.setdefault(\"WANDB_DISABLED\", \"true\")\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "!export CUDA_VISIBLE_DEVICES=0\n",
        "\n",
        "PROJECT_ROOT = pathlib.Path('..').resolve()\n",
        "UTILS_DIR = PROJECT_ROOT / 'utils'\n",
        "DATASET_DIR = PROJECT_ROOT / 'dataset'\n",
        "OUTPUT_ROOT = PROJECT_ROOT / 'outputs'\n",
        "OUTPUT_ROOT.mkdir(exist_ok=True)\n",
        "\n",
        "sys.path.append(str(UTILS_DIR))\n",
        "\n",
        "from train import (\n",
        "    ModelArguments,\n",
        "    DataTrainingArguments,\n",
        "    TrainingArguments,\n",
        "    load_raw_datasets,\n",
        "    prepare_label_mappings,\n",
        ")\n",
        "from dataset_preprocessing import batch_get_preprocessing_function, get_preprocessing_function\n",
        "from model.modeling_utils import DataCollatorForBiEncoder, get_model\n",
        "from clf_trainer import CustomTrainer\n",
        "from progress_logger import LogCallback\n",
        "from model.nGPT_model import NGPTWeightNormCallback\n",
        "from metrics import compute_metrics\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "print(f\"Project root: {PROJECT_ROOT}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1047ff36",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "cf0b4d8b",
      "metadata": {},
      "source": [
        "## 1. ハイパーパラメータとクラス分類器設定\n",
        "`train.sh` のデフォルト値 (学習率・エポック数など) を参考にしつつ、デバッグしやすいようにバッチサイズとサンプル数だけ縮小しています。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "eb817126",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ModelArguments(model_name_or_path='mixedbread-ai/mxbai-embed-large-v1', config_name=None, tokenizer_name=None, cache_dir=None, use_fast_tokenizer=True, model_revision='main', use_auth_token=False, use_flash_attention='eager', device_map='cuda:0', encoding_type='bi_encoder', freeze_encoder=True, classifier_configs='/remote/csifs1/disk3/users/yama11235/yama11235/Sentiment-Circle/outputs/ngpt_classifier_config.json', corr_labels=None, corr_weights=None)\n",
            "DataTrainingArguments(max_seq_length=512, overwrite_cache=False, pad_to_max_length=False, max_train_samples=64, max_eval_samples=64, max_predict_samples=64, train_file=['/remote/csifs1/disk3/users/yama11235/yama11235/Sentiment-Circle/dataset/Train_df.csv'], validation_file=['/remote/csifs1/disk3/users/yama11235/yama11235/Sentiment-Circle/dataset/Valid_df.csv'], test_file=['/remote/csifs1/disk3/users/yama11235/yama11235/Sentiment-Circle/dataset/Test_df.csv'], max_similarity=None, min_similarity=None)\n",
            "TrainingArguments(\n",
            "_n_gpu=1,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "average_tokens_across_devices=False,\n",
            "batch_eval_metrics=False,\n",
            "bf16=True,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_do_concat_batches=True,\n",
            "eval_on_start=False,\n",
            "eval_steps=5,\n",
            "eval_strategy=IntervalStrategy.STEPS,\n",
            "eval_use_gather_object=False,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=None,\n",
            "hub_revision=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_for_metrics=[],\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=0.0001,\n",
            "length_column_name=length,\n",
            "liger_kernel_config=None,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "log_time_interval=15,\n",
            "logging_dir=/remote/csifs1/disk3/users/yama11235/yama11235/Sentiment-Circle/outputs/ngpt_debug_run/runs/Nov17_11-43-40_doremi,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=5,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=SchedulerType.CONSTANT,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=1,\n",
            "optim=OptimizerNames.ADAMW_TORCH,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=/remote/csifs1/disk3/users/yama11235/yama11235/Sentiment-Circle/outputs/ngpt_debug_run,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=64,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=False,\n",
            "report_to=[],\n",
            "restore_callback_states_from_checkpoint=False,\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/remote/csifs1/disk3/users/yama11235/yama11235/Sentiment-Circle/outputs/ngpt_debug_run,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=SaveStrategy.NO,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torch_empty_cache_steps=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_liger_kernel=False,\n",
            "use_mps_device=False,\n",
            "wandb_project=sentiment_circle,\n",
            "wandb_project_name=sentiment_info_nce_ngpt_demo,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "MODEL_NAME = \"mixedbread-ai/mxbai-embed-large-v1\"\n",
        "POOLER_TYPE = \"avg\"\n",
        "MAX_SEQ_LENGTH = 512\n",
        "LEARNING_RATE = 1e-4\n",
        "TRAIN_BATCH_SIZE = 32   # train.sh の 128 だとメモリを圧迫するため縮小\n",
        "EVAL_BATCH_SIZE = 64    # train.sh の 256 から縮小\n",
        "NUM_EPOCHS = 1\n",
        "GRAD_ACCUM = 1\n",
        "LOGGING_STEPS = 5\n",
        "EVAL_STEPS = 5\n",
        "MAX_TRAIN_SAMPLES = 64\n",
        "MAX_EVAL_SAMPLES = 64\n",
        "MAX_PRED_SAMPLES = 64\n",
        "\n",
        "classifier_config = {\n",
        "    \"sentiment\": {\n",
        "        \"type\": \"nGPT\",\n",
        "        \"layer\": -1,\n",
        "        \"objective\": \"infoNCE\",\n",
        "        \"distance\": \"cosine\",\n",
        "        \"pooler_type\": POOLER_TYPE,\n",
        "        \"dropout\": 0.1,\n",
        "        \"bias\": False,\n",
        "        \"base_scale\": 0.03125\n",
        "    }\n",
        "}\n",
        "CLASSIFIER_CONFIG_PATH = OUTPUT_ROOT / \"ngpt_classifier_config.json\"\n",
        "with open(CLASSIFIER_CONFIG_PATH, \"w\") as f:\n",
        "    json.dump(classifier_config, f, indent=2)\n",
        "\n",
        "model_args = ModelArguments(\n",
        "    model_name_or_path=MODEL_NAME,\n",
        "    encoding_type=\"bi_encoder\",\n",
        "    freeze_encoder=True,\n",
        "    device_map=\"cuda:0\",\n",
        "    classifier_configs=str(CLASSIFIER_CONFIG_PATH),\n",
        ")\n",
        "\n",
        "data_args = DataTrainingArguments(\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        "    max_train_samples=MAX_TRAIN_SAMPLES,\n",
        "    max_eval_samples=MAX_EVAL_SAMPLES,\n",
        "    max_predict_samples=MAX_PRED_SAMPLES,\n",
        "    train_file=[str(DATASET_DIR / \"Train_df.csv\")],\n",
        "    validation_file=[str(DATASET_DIR / \"Valid_df.csv\")],\n",
        "    test_file=[str(DATASET_DIR / \"Test_df.csv\")],\n",
        ")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=str(OUTPUT_ROOT / \"ngpt_debug_run\"),\n",
        "    overwrite_output_dir=True,\n",
        "    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
        "    per_device_eval_batch_size=EVAL_BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRAD_ACCUM,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    lr_scheduler_type=\"constant\",\n",
        "    logging_steps=LOGGING_STEPS,\n",
        "    eval_steps=EVAL_STEPS,\n",
        "    eval_strategy=\"steps\",\n",
        "    save_strategy=\"no\",\n",
        "    bf16=True,\n",
        "    do_train=True,\n",
        "    do_eval=True,\n",
        "    do_predict=True,\n",
        "    report_to=[\"none\"],\n",
        "    wandb_project_name=\"sentiment_info_nce_ngpt_demo\",\n",
        "    wandb_project=\"sentiment_circle\",\n",
        "    seed=42,\n",
        ")\n",
        "training_args.remove_unused_columns = False\n",
        "\n",
        "print(model_args)\n",
        "print(data_args)\n",
        "print(training_args)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3626a5e",
      "metadata": {},
      "source": [
        "## 2. データセット読み込み\n",
        "`load_raw_datasets` で `Train/Valid/Test` を読み込み、必要であれば `sentence1` 列へリネームします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "685b78c0",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 11:43:43,344 - train - INFO: Load train files: ['/remote/csifs1/disk3/users/yama11235/yama11235/Sentiment-Circle/dataset/Train_df.csv']\n",
            "2025-11-17 11:43:43,345 - train - INFO: Load validation files: ['/remote/csifs1/disk3/users/yama11235/yama11235/Sentiment-Circle/dataset/Valid_df.csv']\n",
            "2025-11-17 11:43:43,346 - train - INFO: Load test files: ['/remote/csifs1/disk3/users/yama11235/yama11235/Sentiment-Circle/dataset/Test_df.csv']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['sentence1', 'labels'],\n",
            "        num_rows: 64\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['sentence1', 'labels'],\n",
            "        num_rows: 64\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['sentence1', 'labels'],\n",
            "        num_rows: 64\n",
            "    })\n",
            "})\n",
            "sentence3 flag: False\n",
            "{'sentence1': 'is cold and wished to go back to bed', 'labels': 'relief'}\n"
          ]
        }
      ],
      "source": [
        "raw_datasets, sentence3_flag = load_raw_datasets(\n",
        "    model_args=model_args,\n",
        "    data_args=data_args,\n",
        "    training_args=training_args,\n",
        "    seed=training_args.seed,\n",
        ")\n",
        "print(raw_datasets)\n",
        "print(f\"sentence3 flag: {sentence3_flag}\")\n",
        "print(raw_datasets[\"train\"][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27ae8245",
      "metadata": {},
      "source": [
        "## 3. ラベルマッピング & クラス分類器辞書\n",
        "CSV の `labels` 列を `sentiment` に付け替え、`nGPT` 分類器設定を `prepare_label_mappings` に渡します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "c1d10e18",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "labels: ['labels']\n",
            "aspect_key: ['sentiment']\n",
            "classifier configs: {\n",
            "  \"sentiment\": {\n",
            "    \"type\": \"nGPT\",\n",
            "    \"layer\": -1,\n",
            "    \"objective\": \"infoNCE\",\n",
            "    \"distance\": \"cosine\",\n",
            "    \"pooler_type\": \"avg\",\n",
            "    \"dropout\": 0.1,\n",
            "    \"bias\": false,\n",
            "    \"base_scale\": 0.03125\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "(\n",
        "    raw_datasets,\n",
        "    labels,\n",
        "    id2label,\n",
        "    label2id,\n",
        "    aspect_key,\n",
        "    classifier_configs,\n",
        "    classifier_configs_for_trainer,\n",
        "    corr_labels,\n",
        "    corr_weights,\n",
        "    label_name_mappings,\n",
        ") = prepare_label_mappings(\n",
        "    raw_datasets=raw_datasets,\n",
        "    model_args=model_args,\n",
        "    data_args=data_args,\n",
        ")\n",
        "print(f\"labels: {labels}\")\n",
        "print(f\"aspect_key: {aspect_key}\")\n",
        "print(f\"classifier configs: {json.dumps(classifier_configs, indent=2)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0760364",
      "metadata": {},
      "source": [
        "## 4. Config / Tokenizer / モデル (nGPT 判定込み)\n",
        "ここから `train.py` と同様に `AutoConfig` / `AutoTokenizer` をロードし、nGPT ブロック検出によって最適化条件を調整します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "849a82cd",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at mixedbread-ai/mxbai-embed-large-v1 were not used when initializing BertModel: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "2025-11-17 11:44:00,732 - model.modeling_encoders - INFO: Detected nGPT-style classifier block(s); applying initial weight normalization.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "use_ngpt_blocks: True\n"
          ]
        }
      ],
      "source": [
        "config = AutoConfig.from_pretrained(\n",
        "    model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n",
        "    num_labels=len(labels),\n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n",
        "    use_fast=model_args.use_fast_tokenizer,\n",
        ")\n",
        "\n",
        "model_cls = get_model(model_args)\n",
        "config.update(\n",
        "    {\n",
        "        \"freeze_encoder\": model_args.freeze_encoder,\n",
        "        \"model_name_or_path\": model_args.model_name_or_path,\n",
        "        \"attn_implementation\": model_args.use_flash_attention,\n",
        "        \"device_map\": model_args.device_map,\n",
        "    }\n",
        ")\n",
        "labels_for_heads = list(classifier_configs_for_trainer.keys())\n",
        "id2_head = {i: head for i, head in enumerate(labels_for_heads)}\n",
        "model = model_cls(model_config=config, classifier_configs=classifier_configs)\n",
        "\n",
        "if model_args.freeze_encoder:\n",
        "    for param in model.backbone.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "use_ngpt_riemann = bool(getattr(model, \"use_ngpt_blocks\", False))\n",
        "print(f\"use_ngpt_blocks: {use_ngpt_riemann}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a681628",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BiEncoderForClassification(\n",
              "  (backbone): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 1024, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 1024)\n",
              "      (token_type_embeddings): Embedding(2, 1024)\n",
              "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-23): 24 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (cls_pooler): Pooler()\n",
              "  (avg_pooler): Pooler()\n",
              "  (max_pooler): Pooler()\n",
              "  (embedding_classifiers): ModuleDict(\n",
              "    (sentiment): nGPTClassifier(\n",
              "      (transformer): Block(\n",
              "        (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "        (query): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "        (value): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "        (att_c_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "        (c_fc): Linear(in_features=1024, out_features=8192, bias=False)\n",
              "        (silu): SiLU()\n",
              "        (mlp_c_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "      )\n",
              "      (pooler): Pooler()\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mnotebook controller is DISPOSED. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "195d9002",
      "metadata": {},
      "source": [
        "## 5. トークナイズと特徴量生成\n",
        "`get_preprocessing_function` / `batch_get_preprocessing_function` を選び、`DatasetDict.map` で `tokenizer` を実行します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "bb0369c8",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cde416a3cddd40b5afaa73942c0948a1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Running tokenizer on dataset:   0%|          | 0/64 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e57de41185374bdcb7610b6fd46fa69a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Running tokenizer on dataset:   0%|          | 0/64 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ccff582d4ce546daa6a0dda5bf3af030",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Running tokenizer on dataset:   0%|          | 0/64 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'input_ids_2', 'attention_mask_2', 'token_type_ids_2', 'active_heads', 'labels'])\n"
          ]
        }
      ],
      "source": [
        "padding = \"longest\" if data_args.pad_to_max_length else False\n",
        "max_seq_length = min(data_args.max_seq_length, tokenizer.model_max_length)\n",
        "if sentence3_flag:\n",
        "    preprocess_function = batch_get_preprocessing_function(\n",
        "        tokenizer=tokenizer,\n",
        "        sentence1_key=\"sentence1\",\n",
        "        sentence2_key=\"sentence2\",\n",
        "        sentence3_key=\"sentence3\",\n",
        "        sentence3_flag=sentence3_flag,\n",
        "        aspect_key=aspect_key,\n",
        "        padding=padding,\n",
        "        max_seq_length=max_seq_length,\n",
        "        model_args=model_args,\n",
        "        scale=None,\n",
        "    )\n",
        "    batched = True\n",
        "else:\n",
        "    preprocess_function = get_preprocessing_function(\n",
        "        tokenizer=tokenizer,\n",
        "        sentence1_key=\"sentence1\",\n",
        "        sentence2_key=\"sentence2\",\n",
        "        sentence3_key=\"sentence3\",\n",
        "        sentence3_flag=sentence3_flag,\n",
        "        aspect_key=aspect_key,\n",
        "        padding=padding,\n",
        "        max_seq_length=max_seq_length,\n",
        "        model_args=model_args,\n",
        "        scale=None,\n",
        "    )\n",
        "    batched = False\n",
        "\n",
        "processed_datasets = raw_datasets.map(\n",
        "    preprocess_function,\n",
        "    batched=batched,\n",
        "    load_from_cache_file=False,\n",
        "    desc=\"Running tokenizer on dataset\",\n",
        "    remove_columns=raw_datasets[\"train\"].column_names,\n",
        ")\n",
        "train_dataset = processed_datasets[\"train\"]\n",
        "eval_dataset = processed_datasets[\"validation\"]\n",
        "predict_dataset = processed_datasets[\"test\"]\n",
        "print(train_dataset[0].keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "c20761cc",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': [101, 2003, 3147, 1998, 6257, 2000, 2175, 2067, 2000, 2793, 102],\n",
              " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              " 'input_ids_2': None,\n",
              " 'attention_mask_2': None,\n",
              " 'token_type_ids_2': None,\n",
              " 'active_heads': ['sentiment'],\n",
              " 'labels': [8]}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "3936ba8a",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[ 101, 2019, 2250, 4946, 2165, 2125, 1012,  102]], device='cuda:0'), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sent1 = tokenizer(\"An air plane took off.\", return_tensors=\"pt\")\n",
        "sent1.to(device=model.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "a766cc2d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "original_avg: tensor([[ 0.3488, -0.1881, -0.1142,  ..., -0.1884,  0.0338, -0.1004]],\n",
            "       device='cuda:0')\n",
            "torch.Size([1, 1024])\n",
            "norm: 17.202627182006836\n",
            "original_cls: tensor([[ 0.4416,  0.1507, -0.0929,  ..., -0.2336,  0.0686, -0.1392]],\n",
            "       device='cuda:0')\n",
            "torch.Size([1, 1024])\n",
            "norm: 18.008726119995117\n",
            "original_max: tensor([[0.5239, 0.1507, 0.0934,  ..., 0.0707, 0.3726, 0.0795]],\n",
            "       device='cuda:0')\n",
            "torch.Size([1, 1024])\n",
            "norm: 20.008970260620117\n",
            "sentiment: tensor([[ 0.0224, -0.0075, -0.0024,  ..., -0.0129,  0.0038, -0.0013]],\n",
            "       device='cuda:0', grad_fn=<DivBackward0>)\n",
            "torch.Size([1, 1024])\n",
            "norm: 0.9557313919067383\n"
          ]
        }
      ],
      "source": [
        "output = model(\n",
        "    input_ids=sent1[\"input_ids\"],\n",
        "    attention_mask=sent1[\"attention_mask\"],\n",
        ")\n",
        "for k, v in output.items():\n",
        "    print(f\"{k}: {v}\")\n",
        "    print(v.shape)\n",
        "    print(f\"norm: {v.norm()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b67602e",
      "metadata": {},
      "source": [
        "## 6. DataCollator / Trainer 構築\n",
        "`CustomTrainer` を初期化し、nGPT 用の正規化コールバックやメトリクス関数を登録します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "c4727396",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/remote/csifs1/disk3/users/yama11235/yama11235/Sentiment-Circle/utils/clf_trainer.py:71: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
            "  super().__init__(*args, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "collator_dtype = getattr(config, \"torch_dtype\", torch.float32)\n",
        "data_collator = DataCollatorForBiEncoder(\n",
        "    tokenizer=tokenizer,\n",
        "    padding=\"max_length\",\n",
        "    pad_to_multiple_of=None,\n",
        "    dtype=collator_dtype,\n",
        ")\n",
        "\n",
        "trainer_ref = {\"trainer\": None}\n",
        "\n",
        "def train_centroid_getter():\n",
        "    trainer_obj = trainer_ref[\"trainer\"]\n",
        "    if trainer_obj is None:\n",
        "        return {}\n",
        "    return trainer_obj.get_train_label_centroids()\n",
        "\n",
        "def compute_fn(eval_pred):\n",
        "    trainer_obj = trainer_ref[\"trainer\"]\n",
        "    embedding_mode = \"classifier\"\n",
        "    if trainer_obj is not None and getattr(trainer_obj, \"use_original_eval_embeddings\", False):\n",
        "        embedding_mode = \"original\"\n",
        "    return compute_metrics(\n",
        "        eval_pred,\n",
        "        classifier_configs=classifier_configs_for_trainer,\n",
        "        id2_head=id2_head,\n",
        "        train_centroid_getter=train_centroid_getter,\n",
        "        embedding_eval_mode=embedding_mode,\n",
        "    )\n",
        "\n",
        "ngpt_callback = NGPTWeightNormCallback(enabled=use_ngpt_riemann)\n",
        "trainer = CustomTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    classifier_configs=classifier_configs_for_trainer,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    compute_metrics=compute_fn,\n",
        "    tokenizer=tokenizer,\n",
        "    callbacks=[LogCallback, ngpt_callback],\n",
        "    dtype=collator_dtype,\n",
        "    corr_labels=corr_labels,\n",
        "    corr_weights=corr_weights,\n",
        "    tsne_save_dir=os.path.join(training_args.output_dir, \"tsne_plots\"),\n",
        "    tsne_label_mappings=label_name_mappings,\n",
        ")\n",
        "trainer_ref[\"trainer\"] = trainer\n",
        "trainer\n",
        "trainer.remove_callback(PrinterCallback)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "8f9ed8e5",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'sentiment': <utils.head_objectives.InfoNCEObjective at 0x9024b49cda0>}"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.head_objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "461f82b2",
      "metadata": {},
      "source": [
        "## 7. 評価→学習→テスト\n",
        "`train.py` と同様に、初期 `evaluate` → `train` → `test (evaluate on test split)` の順に実行してログを確認します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "e8ade2c6",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 11:46:05,489 - progress_logger - INFO: {'eval_loss': 2.4146645069122314, 'eval_model_preparation_time': 0.0031, 'eval_sentiment_knn_accuracy': 0.390625, 'eval_sentiment_knn_macro_f1': 0.28509300960575323, 'eval_sentiment_cluster_ami': 0.1370958440358634, 'eval_sentiment_cluster_v_measure': 0.369443699131842, 'eval_runtime': 0.4781, 'eval_samples_per_second': 133.855, 'eval_steps_per_second': 2.091}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'eval_loss': 2.4146645069122314,\n",
              " 'eval_model_preparation_time': 0.0031,\n",
              " 'eval_sentiment_knn_accuracy': 0.390625,\n",
              " 'eval_sentiment_knn_macro_f1': 0.28509300960575323,\n",
              " 'eval_sentiment_cluster_ami': 0.1370958440358634,\n",
              " 'eval_sentiment_cluster_v_measure': 0.369443699131842,\n",
              " 'eval_runtime': 0.4781,\n",
              " 'eval_samples_per_second': 133.855,\n",
              " 'eval_steps_per_second': 2.091}"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "baseline_metrics = trainer.evaluate(eval_dataset=eval_dataset)\n",
        "baseline_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_result = trainer.train()\n",
        "train_metrics = train_result.metrics\n",
        "train_metrics[\"train_samples\"] = len(train_dataset)\n",
        "train_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_metrics = trainer.evaluate(eval_dataset=predict_dataset)\n",
        "test_metrics"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "my-project",
      "language": "python",
      "name": "my-project"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
