{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "99bbf77a",
      "metadata": {},
      "source": [
        "# train.py フロー検証 (nGPT classifier)\n",
        "\n",
        "train.pyの実行ステップを1つずつ追跡しながらnGPT分類器をテストできる検証ノートブックです。\n",
        "\n",
        "**リファクタリング後のモジュール構造に対応（修正版）**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a4225bb",
      "metadata": {},
      "source": [
        "## 0. 準備とインポート"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "c61daafd",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Project root: /remote/csifs1/disk3/users/yama11235/yama11235/Sentiment-Circle\n",
            "CUDA available: True\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "import pathlib\n",
        "import random\n",
        "import sys\n",
        "from functools import partial\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoConfig, AutoTokenizer, PrinterCallback\n",
        "\n",
        "# 環境変数設定\n",
        "os.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\n",
        "os.environ.setdefault(\"WANDB_MODE\", \"disabled\")\n",
        "os.environ.setdefault(\"WANDB_DISABLED\", \"true\")\n",
        "\n",
        "# パス設定\n",
        "PROJECT_ROOT = pathlib.Path('..').resolve()\n",
        "UTILS_DIR = PROJECT_ROOT / 'utils'\n",
        "DATASET_DIR = PROJECT_ROOT / 'dataset'\n",
        "OUTPUT_ROOT = PROJECT_ROOT / 'outputs'\n",
        "OUTPUT_ROOT.mkdir(exist_ok=True)\n",
        "\n",
        "# パスに追加（後方互換性のため）\n",
        "sys.path.insert(0, str(PROJECT_ROOT))\n",
        "sys.path.insert(0, str(UTILS_DIR))\n",
        "\n",
        "# リファクタリング後のインポート\n",
        "from utils.config import ModelArguments, DataTrainingArguments, TrainingArguments\n",
        "from utils.data import load_raw_datasets, prepare_label_mappings\n",
        "from utils.training import (\n",
        "    setup_model_and_config,\n",
        "    setup_tokenizer,\n",
        "    prepare_datasets,\n",
        "    create_trainer,\n",
        ")\n",
        "\n",
        "# ロギング設定\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# シード設定\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "print(f\"Project root: {PROJECT_ROOT}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e840133c",
      "metadata": {},
      "source": [
        "## 1. 引数の設定\n",
        "\n",
        "train.pyと同じようにModelArguments、DataTrainingArguments、TrainingArgumentsを設定します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "1c4408a8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Model: mixedbread-ai/mxbai-embed-large-v1\n",
            "✓ Encoding type: bi_encoder\n",
            "✓ Max seq length: 512\n",
            "✓ Train samples: 64\n",
            "✓ Batch size: 4\n",
            "✓ Learning rate: 2e-05\n",
            "✓ Epochs: 3\n"
          ]
        }
      ],
      "source": [
        "# モデル引数\n",
        "model_args = ModelArguments(\n",
        "    model_name_or_path='mixedbread-ai/mxbai-embed-large-v1',\n",
        "    encoding_type='bi_encoder',\n",
        "    freeze_encoder=True,\n",
        "    classifier_configs=str(OUTPUT_ROOT / 'ngpt_classifier_config.json'),\n",
        "    device_map='cuda:0',\n",
        ")\n",
        "\n",
        "# データ引数\n",
        "data_args = DataTrainingArguments(\n",
        "    max_seq_length=512,\n",
        "    max_train_samples=64,\n",
        "    max_eval_samples=64,\n",
        "    max_predict_samples=64,\n",
        "    train_file=[str(DATASET_DIR / 'Train_df.csv')],\n",
        "    validation_file=[str(DATASET_DIR / 'Valid_df.csv')],\n",
        "    test_file=[str(DATASET_DIR / 'Test_df.csv')],\n",
        ")\n",
        "\n",
        "# トレーニング引数\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=str(OUTPUT_ROOT / 'ngpt_test'),\n",
        "    do_train=True,\n",
        "    do_eval=True,\n",
        "    do_predict=True,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=64,\n",
        "    gradient_accumulation_steps=2,\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=3,\n",
        "    warmup_ratio=0.1,\n",
        "    logging_steps=10,\n",
        "    save_steps=50,\n",
        "    eval_steps=50,\n",
        "    eval_strategy='steps',\n",
        "    save_strategy='steps',\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model='eval_loss',\n",
        "    greater_is_better=False,\n",
        "    bf16=True if torch.cuda.is_bf16_supported() else False,\n",
        "    fp16=False if torch.cuda.is_bf16_supported() else True,\n",
        "    seed=seed,\n",
        "    report_to=[],\n",
        "    remove_unused_columns=False,\n",
        ")\n",
        "\n",
        "print(f\"✓ Model: {model_args.model_name_or_path}\")\n",
        "print(f\"✓ Encoding type: {model_args.encoding_type}\")\n",
        "print(f\"✓ Max seq length: {data_args.max_seq_length}\")\n",
        "print(f\"✓ Train samples: {data_args.max_train_samples}\")\n",
        "print(f\"✓ Batch size: {training_args.per_device_train_batch_size}\")\n",
        "print(f\"✓ Learning rate: {training_args.learning_rate}\")\n",
        "print(f\"✓ Epochs: {training_args.num_train_epochs}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0cd4b08e",
      "metadata": {},
      "source": [
        "## 2. データセット読み込み\n",
        "\n",
        "`load_raw_datasets`関数を使用してデータセットを読み込みます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "dbd2f7ad",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 14:27:23,104 - utils.data.data_loader - INFO: Load train files: ['/remote/csifs1/disk3/users/yama11235/yama11235/Sentiment-Circle/dataset/Train_df.csv']\n",
            "2025-11-17 14:27:23,105 - utils.data.data_loader - INFO: Load validation files: ['/remote/csifs1/disk3/users/yama11235/yama11235/Sentiment-Circle/dataset/Valid_df.csv']\n",
            "2025-11-17 14:27:23,106 - utils.data.data_loader - INFO: Load test files: ['/remote/csifs1/disk3/users/yama11235/yama11235/Sentiment-Circle/dataset/Test_df.csv']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Raw datasets loaded:\n",
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['sentence1', 'labels'],\n",
            "        num_rows: 64\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['sentence1', 'labels'],\n",
            "        num_rows: 64\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['sentence1', 'labels'],\n",
            "        num_rows: 64\n",
            "    })\n",
            "})\n",
            "\n",
            "sentence3_flag: False\n",
            "\n",
            "Train: 64 samples\n",
            "Validation: 64 samples\n",
            "Test: 64 samples\n",
            "\n",
            "Sample from train:\n",
            "{'sentence1': 'is cold and wished to go back to bed', 'labels': 'relief'}\n"
          ]
        }
      ],
      "source": [
        "# データセット読み込み（train.pyと同じ引数）\n",
        "raw_datasets, sentence3_flag = load_raw_datasets(\n",
        "    model_args=model_args,\n",
        "    data_args=data_args,\n",
        "    training_args=training_args,\n",
        "    seed=seed,\n",
        ")\n",
        "\n",
        "print(\"\\nRaw datasets loaded:\")\n",
        "print(raw_datasets)\n",
        "print(f\"\\nsentence3_flag: {sentence3_flag}\")\n",
        "print(f\"\\nTrain: {len(raw_datasets['train'])} samples\")\n",
        "print(f\"Validation: {len(raw_datasets['validation'])} samples\")\n",
        "print(f\"Test: {len(raw_datasets['test'])} samples\")\n",
        "\n",
        "# サンプル確認\n",
        "print(\"\\nSample from train:\")\n",
        "print(raw_datasets['train'][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "937c8c91",
      "metadata": {},
      "source": [
        "## 3. ラベルマッピングの準備\n",
        "\n",
        "`prepare_label_mappings`関数でラベルマッピングと分類器設定を準備します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "04bc97b6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Label mappings prepared:\n",
            "Labels: ['labels']\n",
            "Aspect keys: ['sentiment']\n",
            "\n",
            "Classifier configs:\n",
            "  sentiment: nGPT\n",
            "\n",
            "Label name mappings:\n",
            "{\n",
            "  \"sentiment\": {\n",
            "    \"0\": \"anger\",\n",
            "    \"1\": \"boredom\",\n",
            "    \"2\": \"disgust\",\n",
            "    \"3\": \"excitement\",\n",
            "    \"4\": \"fear\",\n",
            "    \"5\": \"gratitude\",\n",
            "    \"6\": \"joy\",\n",
            "    \"7\": \"optimism\",\n",
            "    \"8\": \"relief\",\n",
            "    \"9\": \"sadness\",\n",
            "    \"10\": \"surprise\"\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# ラベルマッピング準備（train.pyと同じ引数）\n",
        "(\n",
        "    raw_datasets,\n",
        "    labels,\n",
        "    id2label,\n",
        "    label2id,\n",
        "    aspect_key,\n",
        "    classifier_configs,\n",
        "    classifier_configs_for_trainer,\n",
        "    corr_labels,\n",
        "    corr_weights,\n",
        "    label_name_mappings,\n",
        ") = prepare_label_mappings(\n",
        "    raw_datasets=raw_datasets,\n",
        "    model_args=model_args,\n",
        "    data_args=data_args,\n",
        ")\n",
        "\n",
        "print(\"\\nLabel mappings prepared:\")\n",
        "print(f\"Labels: {labels}\")\n",
        "print(f\"Aspect keys: {aspect_key}\")\n",
        "print(f\"\\nClassifier configs:\")\n",
        "for name, config in classifier_configs.items():\n",
        "    print(f\"  {name}: {config.get('type', 'N/A')}\")\n",
        "print(f\"\\nLabel name mappings:\")\n",
        "print(json.dumps(label_name_mappings, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5cba0d6a",
      "metadata": {},
      "source": [
        "## 4. モデルと設定のセットアップ\n",
        "\n",
        "`setup_model_and_config`関数でモデルとconfigを初期化します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "9951b258",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at mixedbread-ai/mxbai-embed-large-v1 were not used when initializing BertModel: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "2025-11-17 14:27:30,475 - utils.model.modeling_encoders - INFO: Detected nGPT-style classifier block(s); applying initial weight normalization.\n",
            "2025-11-17 14:27:30,516 - utils.training.train_setup - INFO: nGPT-style classifier detected. Enabling pseudo-Riemann weight normalization and nGPT-friendly optimizer settings.\n",
            "2025-11-17 14:27:30,517 - utils.training.train_setup - WARNING: Overriding warmup_ratio from 0.1 to 0.0 for nGPT.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Model: BiEncoderForClassification\n",
            "Config: BertConfig\n",
            "Use nGPT Riemann: True\n",
            "\n",
            "Total parameters: 350,880,768\n",
            "Trainable parameters: 16,788,480\n"
          ]
        }
      ],
      "source": [
        "# モデルとconfig設定（train.pyと同じ引数）\n",
        "config, model, use_ngpt_riemann = setup_model_and_config(\n",
        "    model_args=model_args,\n",
        "    training_args=training_args,\n",
        "    labels=list(classifier_configs_for_trainer.keys()),\n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        "    classifier_configs=classifier_configs,\n",
        ")\n",
        "\n",
        "print(f\"\\nModel: {model.__class__.__name__}\")\n",
        "print(f\"Config: {config.__class__.__name__}\")\n",
        "print(f\"Use nGPT Riemann: {use_ngpt_riemann}\")\n",
        "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afc0765a",
      "metadata": {},
      "source": [
        "### モデルのアーキテクチャ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "bb5cc4ff",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BiEncoderForClassification(\n",
              "  (backbone): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 1024, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 1024)\n",
              "      (token_type_embeddings): Embedding(2, 1024)\n",
              "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-23): 24 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (cls_pooler): Pooler()\n",
              "  (avg_pooler): Pooler()\n",
              "  (max_pooler): Pooler()\n",
              "  (embedding_classifiers): ModuleDict(\n",
              "    (sentiment): nGPTClassifier(\n",
              "      (transformer): Block(\n",
              "        (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "        (query): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "        (value): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "        (att_c_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "        (c_fc): Linear(in_features=1024, out_features=8192, bias=False)\n",
              "        (silu): SiLU()\n",
              "        (mlp_c_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "      )\n",
              "      (pooler): Pooler()\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13bad87b",
      "metadata": {},
      "source": [
        "## 5. トークナイザーのセットアップ\n",
        "\n",
        "`setup_tokenizer`関数でトークナイザーを初期化します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "e3563438",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Tokenizer: BertTokenizerFast\n",
            "Vocab size: 30522\n",
            "Model max length: 512\n"
          ]
        }
      ],
      "source": [
        "# トークナイザー設定（train.pyと同じ引数）\n",
        "tokenizer = setup_tokenizer(model_args)\n",
        "\n",
        "print(f\"\\nTokenizer: {tokenizer.__class__.__name__}\")\n",
        "print(f\"Vocab size: {len(tokenizer)}\")\n",
        "print(f\"Model max length: {tokenizer.model_max_length}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56993995",
      "metadata": {},
      "source": [
        "## 6. データセットの前処理\n",
        "\n",
        "`prepare_datasets`関数でトークナイズと前処理を適用します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "7a8ec600",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a97a80ec5550417fa5c2c6db9a23485c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Running tokenizer on dataset:   0%|          | 0/64 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 14:27:37,190 - utils.training.train_setup - INFO: tokens: [CLS] thank you! forgot to add her youtube. and her second, smaller, channels ( super entertaining rants ) : [SEP]\n",
            "2025-11-17 14:27:37,191 - utils.training.train_setup - INFO: Sample 14 of the training set: {'input_ids': [101, 4067, 2017, 999, 9471, 2000, 5587, 2014, 7858, 1012, 1998, 2014, 2117, 1010, 3760, 1010, 6833, 1006, 3565, 14036, 2743, 3215, 1007, 1024, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'input_ids_2': None, 'attention_mask_2': None, 'token_type_ids_2': None, 'active_heads': ['sentiment'], 'labels': [5]}.\n",
            "2025-11-17 14:27:37,192 - utils.training.train_setup - INFO: tokens: [CLS] they all clapped. [SEP]\n",
            "2025-11-17 14:27:37,193 - utils.training.train_setup - INFO: Sample 3 of the training set: {'input_ids': [101, 2027, 2035, 18310, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1], 'input_ids_2': None, 'attention_mask_2': None, 'token_type_ids_2': None, 'active_heads': ['sentiment'], 'labels': [6]}.\n",
            "2025-11-17 14:27:37,193 - utils.training.train_setup - INFO: tokens: [CLS] oh i see. thanks for letting me know. i fixed it now. the real link is here [SEP]\n",
            "2025-11-17 14:27:37,194 - utils.training.train_setup - INFO: Sample 35 of the training set: {'input_ids': [101, 2821, 1045, 2156, 1012, 4283, 2005, 5599, 2033, 2113, 1012, 1045, 4964, 2009, 2085, 1012, 1996, 2613, 4957, 2003, 2182, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'input_ids_2': None, 'attention_mask_2': None, 'token_type_ids_2': None, 'active_heads': ['sentiment'], 'labels': [5]}.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Preprocessed datasets:\n",
            "Train: 64 samples\n",
            "Eval: 64 samples\n",
            "Test: 64 samples\n",
            "Max train samples: 64\n",
            "\n",
            "Preprocessed sample keys:\n",
            "['input_ids', 'token_type_ids', 'attention_mask', 'input_ids_2', 'attention_mask_2', 'token_type_ids_2', 'active_heads', 'labels']\n"
          ]
        }
      ],
      "source": [
        "# データセット前処理（train.pyと同じ引数）\n",
        "train_dataset, eval_dataset, predict_dataset, max_train_samples = prepare_datasets(\n",
        "    raw_datasets=raw_datasets,\n",
        "    tokenizer=tokenizer,\n",
        "    data_args=data_args,\n",
        "    model_args=model_args,\n",
        "    training_args=training_args,\n",
        "    aspect_key=aspect_key,\n",
        "    sentence3_flag=sentence3_flag,\n",
        ")\n",
        "\n",
        "print(f\"\\nPreprocessed datasets:\")\n",
        "print(f\"Train: {len(train_dataset)} samples\")\n",
        "print(f\"Eval: {len(eval_dataset)} samples\")\n",
        "print(f\"Test: {len(predict_dataset)} samples\")\n",
        "print(f\"Max train samples: {max_train_samples}\")\n",
        "\n",
        "# サンプル確認\n",
        "print(\"\\nPreprocessed sample keys:\")\n",
        "print(list(train_dataset[0].keys()))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c8f57ea",
      "metadata": {},
      "source": [
        "### 推論テスト"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "60ee3e9e",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'original_avg': tensor([[ 0.3068, -0.1795, -0.2015,  ...,  0.3230, -0.2399, -0.2997]],\n",
              "        device='cuda:0'),\n",
              " 'original_cls': tensor([[ 0.2106, -0.0967, -0.1799,  ...,  0.4371, -0.1756, -0.3231]],\n",
              "        device='cuda:0'),\n",
              " 'original_max': tensor([[ 0.6953, -0.0715,  0.1926,  ...,  0.5316,  0.0780,  0.0718]],\n",
              "        device='cuda:0'),\n",
              " 'sentiment': tensor([[ 0.0185, -0.0078, -0.0117,  ...,  0.0192, -0.0134, -0.0153]],\n",
              "        device='cuda:0', grad_fn=<DivBackward0>)}"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_case = tokenizer(\"This is a test sentence for inference.\", return_tensors=\"pt\")\n",
        "model(input_ids=test_case['input_ids'].to(model_args.device_map), attention_mask=test_case['attention_mask'].to(model_args.device_map))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24e13aa4",
      "metadata": {},
      "source": [
        "## 7. トレーナーの作成\n",
        "\n",
        "`create_trainer`関数でCustomTrainerを初期化します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "ecb0ac68",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✓ Trainer initialized successfully\n",
            "Model device: cuda:0\n",
            "\n",
            "Head objectives:\n",
            "  sentiment: InfoNCEObjective\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/remote/csifs1/disk3/users/yama11235/yama11235/Sentiment-Circle/utils/clf_trainer.py:65: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
            "  super().__init__(*args, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "# トレーナー作成（train.pyと同じ引数）\n",
        "id2_head = {i: head for i, head in enumerate(classifier_configs_for_trainer.keys())}\n",
        "\n",
        "trainer, trainer_state = create_trainer(\n",
        "    model=model,\n",
        "    config=config,\n",
        "    training_args=training_args,\n",
        "    classifier_configs_for_trainer=classifier_configs_for_trainer,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    corr_labels=corr_labels,\n",
        "    corr_weights=corr_weights,\n",
        "    label_name_mappings=label_name_mappings,\n",
        "    use_ngpt_riemann=use_ngpt_riemann,\n",
        "    id2_head=id2_head,\n",
        ")\n",
        "\n",
        "# PrinterCallbackを削除（ノートブック環境用）\n",
        "trainer.remove_callback(PrinterCallback)\n",
        "\n",
        "print(\"\\n✓ Trainer initialized successfully\")\n",
        "print(f\"Model device: {trainer.model.device}\")\n",
        "print(f\"\\nHead objectives:\")\n",
        "for head_name, objective in trainer.head_objectives.items():\n",
        "    print(f\"  {head_name}: {objective.__class__.__name__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b17fe0a",
      "metadata": {},
      "source": [
        "## 8. 初期評価（ベースライン）\n",
        "\n",
        "トレーニング前の初期性能を確認します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "2aa26120",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running baseline evaluation...\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Caught RuntimeError in replica 1 on device 1.\nOriginal Traceback (most recent call last):\n  File \"/remote/csifs1/disk3/users/yama11235/yama11235/SLBERT/my-project/.venv/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py\", line 97, in _worker\n    output = module(*input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/remote/csifs1/disk3/users/yama11235/yama11235/SLBERT/my-project/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/remote/csifs1/disk3/users/yama11235/yama11235/SLBERT/my-project/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/remote/csifs1/disk3/users/yama11235/yama11235/Sentiment-Circle/utils/model/modeling_encoders.py\", line 222, in forward\n    outputs = self._paths[uniform].run_full(batch_inputs, extra_kwargs)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/remote/csifs1/disk3/users/yama11235/yama11235/Sentiment-Circle/utils/model/sentence_paths.py\", line 45, in run_full\n    return self._forward(batch, extra_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/remote/csifs1/disk3/users/yama11235/yama11235/Sentiment-Circle/utils/model/sentence_paths.py\", line 68, in _forward\n    return self.model.encode(**args)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/remote/csifs1/disk3/users/yama11235/yama11235/Sentiment-Circle/utils/model/modeling_encoders.py\", line 255, in encode\n    outputs = self.backbone(\n              ^^^^^^^^^^^^^^\n  File \"/remote/csifs1/disk3/users/yama11235/yama11235/SLBERT/my-project/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/remote/csifs1/disk3/users/yama11235/yama11235/SLBERT/my-project/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/remote/csifs1/disk3/users/yama11235/yama11235/SLBERT/my-project/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py\", line 932, in forward\n    embedding_output = self.embeddings(\n                       ^^^^^^^^^^^^^^^^\n  File \"/remote/csifs1/disk3/users/yama11235/yama11235/SLBERT/my-project/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/remote/csifs1/disk3/users/yama11235/yama11235/SLBERT/my-project/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/remote/csifs1/disk3/users/yama11235/yama11235/SLBERT/my-project/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py\", line 179, in forward\n    inputs_embeds = self.word_embeddings(input_ids)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/remote/csifs1/disk3/users/yama11235/yama11235/SLBERT/my-project/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/remote/csifs1/disk3/users/yama11235/yama11235/SLBERT/my-project/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/remote/csifs1/disk3/users/yama11235/yama11235/SLBERT/my-project/.venv/lib/python3.12/site-packages/torch/nn/modules/sparse.py\", line 190, in forward\n    return F.embedding(\n           ^^^^^^^^^^^^\n  File \"/remote/csifs1/disk3/users/yama11235/yama11235/SLBERT/my-project/.venv/lib/python3.12/site-packages/torch/nn/functional.py\", line 2551, in embedding\n    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cuda:1! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRunning baseline evaluation...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m baseline_metrics = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m80\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBASELINE METRICS\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/remote/csifs1/disk3/users/yama11235/yama11235/Sentiment-Circle/utils/clf_trainer.py:219\u001b[39m, in \u001b[36mCustomTrainer.evaluate\u001b[39m\u001b[34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[39m\n\u001b[32m    217\u001b[39m \u001b[38;5;28mself\u001b[39m._current_eval_embedding_mode = \u001b[33m\"\u001b[39m\u001b[33moriginal\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_original_now \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mclassifier\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    218\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[43m        \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    225\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._current_eval_embedding_mode == \u001b[33m\"\u001b[39m\u001b[33moriginal\u001b[39m\u001b[33m\"\u001b[39m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/remote/csifs1/disk3/users/yama11235/yama11235/SLBERT/my-project/.venv/lib/python3.12/site-packages/transformers/trainer.py:4200\u001b[39m, in \u001b[36mTrainer.evaluate\u001b[39m\u001b[34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[39m\n\u001b[32m   4197\u001b[39m start_time = time.time()\n\u001b[32m   4199\u001b[39m eval_loop = \u001b[38;5;28mself\u001b[39m.prediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.use_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.evaluation_loop\n\u001b[32m-> \u001b[39m\u001b[32m4200\u001b[39m output = \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4201\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4202\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mEvaluation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4203\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[32m   4204\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[32m   4205\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   4206\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4207\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4208\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4210\u001b[39m total_batch_size = \u001b[38;5;28mself\u001b[39m.args.eval_batch_size * \u001b[38;5;28mself\u001b[39m.args.world_size\n\u001b[32m   4211\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_jit_compilation_time\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output.metrics:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/remote/csifs1/disk3/users/yama11235/yama11235/Sentiment-Circle/utils/clf_trainer.py:231\u001b[39m, in \u001b[36mCustomTrainer.evaluation_loop\u001b[39m\u001b[34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[39m\n\u001b[32m    230\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mevaluation_loop\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataloader, description: \u001b[38;5;28mstr\u001b[39m, prediction_loss_only: Optional[\u001b[38;5;28mbool\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m, ignore_keys: Optional[List[\u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m, metric_key_prefix: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33meval\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m231\u001b[39m     output = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluation_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\n\u001b[32m    233\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    234\u001b[39m     \u001b[38;5;28mself\u001b[39m._last_eval_predictions = output.predictions\n\u001b[32m    235\u001b[39m     \u001b[38;5;28mself\u001b[39m._last_eval_label_ids = output.label_ids\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/remote/csifs1/disk3/users/yama11235/yama11235/SLBERT/my-project/.venv/lib/python3.12/site-packages/transformers/trainer.py:4395\u001b[39m, in \u001b[36mTrainer.evaluation_loop\u001b[39m\u001b[34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[39m\n\u001b[32m   4392\u001b[39m         batch_size = observed_batch_size\n\u001b[32m   4394\u001b[39m \u001b[38;5;66;03m# Prediction step\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4395\u001b[39m losses, logits, labels = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprediction_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4396\u001b[39m main_input_name = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.model, \u001b[33m\"\u001b[39m\u001b[33mmain_input_name\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   4397\u001b[39m inputs_decode = (\n\u001b[32m   4398\u001b[39m     \u001b[38;5;28mself\u001b[39m._prepare_input(inputs[main_input_name]) \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33minputs\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m args.include_for_metrics \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4399\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/remote/csifs1/disk3/users/yama11235/yama11235/Sentiment-Circle/utils/clf_trainer.py:204\u001b[39m, in \u001b[36mCustomTrainer.prediction_step\u001b[39m\u001b[34m(self, model, inputs, prediction_loss_only, ignore_keys)\u001b[39m\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprediction_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, inputs, prediction_loss_only, ignore_keys=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m204\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mevaluation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/remote/csifs1/disk3/users/yama11235/yama11235/Sentiment-Circle/utils/clf_trainer.py:193\u001b[39m, in \u001b[36mCustomTrainer.evaluation_step\u001b[39m\u001b[34m(self, model, inputs, prediction_loss_only, ignore_keys)\u001b[39m\n\u001b[32m    191\u001b[39m model.eval()\n\u001b[32m    192\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m193\u001b[39m     loss, logits = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    195\u001b[39m labels_tensor = inputs[\u001b[33m\"\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    196\u001b[39m ah_list = flatten_strings(inputs[\u001b[33m\"\u001b[39m\u001b[33mactive_heads\u001b[39m\u001b[33m\"\u001b[39m])\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/remote/csifs1/disk3/users/yama11235/yama11235/Sentiment-Circle/utils/clf_trainer.py:154\u001b[39m, in \u001b[36mCustomTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, **kwargs)\u001b[39m\n\u001b[32m    151\u001b[39m active_heads = extract_unique_strings(inputs[\u001b[33m\"\u001b[39m\u001b[33mactive_heads\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    152\u001b[39m total_loss = torch.tensor(\u001b[32m0.0\u001b[39m, device=device, requires_grad=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m single_loss, outputs1 = \u001b[43mcompute_single_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactive_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m single_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    156\u001b[39m     total_loss = total_loss + single_loss\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/remote/csifs1/disk3/users/yama11235/yama11235/Sentiment-Circle/utils/loss_function.py:37\u001b[39m, in \u001b[36mcompute_single_loss\u001b[39m\u001b[34m(trainer, model, inputs, active_heads, device)\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inputs:\n\u001b[32m     35\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, {}\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m loss: Optional[torch.Tensor] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     39\u001b[39m labels = inputs.get(\u001b[33m\"\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/remote/csifs1/disk3/users/yama11235/yama11235/SLBERT/my-project/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/remote/csifs1/disk3/users/yama11235/yama11235/SLBERT/my-project/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/remote/csifs1/disk3/users/yama11235/yama11235/SLBERT/my-project/.venv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py:194\u001b[39m, in \u001b[36mDataParallel.forward\u001b[39m\u001b[34m(self, *inputs, **kwargs)\u001b[39m\n\u001b[32m    192\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.module(*inputs[\u001b[32m0\u001b[39m], **module_kwargs[\u001b[32m0\u001b[39m])\n\u001b[32m    193\u001b[39m replicas = \u001b[38;5;28mself\u001b[39m.replicate(\u001b[38;5;28mself\u001b[39m.module, \u001b[38;5;28mself\u001b[39m.device_ids[: \u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.gather(outputs, \u001b[38;5;28mself\u001b[39m.output_device)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/remote/csifs1/disk3/users/yama11235/yama11235/SLBERT/my-project/.venv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py:213\u001b[39m, in \u001b[36mDataParallel.parallel_apply\u001b[39m\u001b[34m(self, replicas, inputs, kwargs)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparallel_apply\u001b[39m(\n\u001b[32m    211\u001b[39m     \u001b[38;5;28mself\u001b[39m, replicas: Sequence[T], inputs: Sequence[Any], kwargs: Any\n\u001b[32m    212\u001b[39m ) -> \u001b[38;5;28mlist\u001b[39m[Any]:\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/remote/csifs1/disk3/users/yama11235/yama11235/SLBERT/my-project/.venv/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:127\u001b[39m, in \u001b[36mparallel_apply\u001b[39m\u001b[34m(modules, inputs, kwargs_tup, devices)\u001b[39m\n\u001b[32m    125\u001b[39m     output = results[i]\n\u001b[32m    126\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ExceptionWrapper):\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m         \u001b[43moutput\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    128\u001b[39m     outputs.append(output)\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/remote/csifs1/disk3/users/yama11235/yama11235/SLBERT/my-project/.venv/lib/python3.12/site-packages/torch/_utils.py:750\u001b[39m, in \u001b[36mExceptionWrapper.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    746\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    747\u001b[39m     \u001b[38;5;66;03m# If the exception takes multiple arguments or otherwise can't\u001b[39;00m\n\u001b[32m    748\u001b[39m     \u001b[38;5;66;03m# be constructed, don't try to instantiate since we don't know how to\u001b[39;00m\n\u001b[32m    749\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m750\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
            "\u001b[31mRuntimeError\u001b[39m: Caught RuntimeError in replica 1 on device 1.\nOriginal Traceback (most recent call last):\n  File \"/remote/csifs1/disk3/users/yama11235/yama11235/SLBERT/my-project/.venv/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py\", line 97, in _worker\n    output = module(*input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/remote/csifs1/disk3/users/yama11235/yama11235/SLBERT/my-project/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/remote/csifs1/disk3/users/yama11235/yama11235/SLBERT/my-project/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/remote/csifs1/disk3/users/yama11235/yama11235/Sentiment-Circle/utils/model/modeling_encoders.py\", line 222, in forward\n    outputs = self._paths[uniform].run_full(batch_inputs, extra_kwargs)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/remote/csifs1/disk3/users/yama11235/yama11235/Sentiment-Circle/utils/model/sentence_paths.py\", line 45, in run_full\n    return self._forward(batch, extra_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/remote/csifs1/disk3/users/yama11235/yama11235/Sentiment-Circle/utils/model/sentence_paths.py\", line 68, in _forward\n    return self.model.encode(**args)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/remote/csifs1/disk3/users/yama11235/yama11235/Sentiment-Circle/utils/model/modeling_encoders.py\", line 255, in encode\n    outputs = self.backbone(\n              ^^^^^^^^^^^^^^\n  File \"/remote/csifs1/disk3/users/yama11235/yama11235/SLBERT/my-project/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/remote/csifs1/disk3/users/yama11235/yama11235/SLBERT/my-project/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/remote/csifs1/disk3/users/yama11235/yama11235/SLBERT/my-project/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py\", line 932, in forward\n    embedding_output = self.embeddings(\n                       ^^^^^^^^^^^^^^^^\n  File \"/remote/csifs1/disk3/users/yama11235/yama11235/SLBERT/my-project/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/remote/csifs1/disk3/users/yama11235/yama11235/SLBERT/my-project/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/remote/csifs1/disk3/users/yama11235/yama11235/SLBERT/my-project/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py\", line 179, in forward\n    inputs_embeds = self.word_embeddings(input_ids)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/remote/csifs1/disk3/users/yama11235/yama11235/SLBERT/my-project/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/remote/csifs1/disk3/users/yama11235/yama11235/SLBERT/my-project/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/remote/csifs1/disk3/users/yama11235/yama11235/SLBERT/my-project/.venv/lib/python3.12/site-packages/torch/nn/modules/sparse.py\", line 190, in forward\n    return F.embedding(\n           ^^^^^^^^^^^^\n  File \"/remote/csifs1/disk3/users/yama11235/yama11235/SLBERT/my-project/.venv/lib/python3.12/site-packages/torch/nn/functional.py\", line 2551, in embedding\n    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cuda:1! (when checking argument for argument index in method wrapper_CUDA__index_select)\n"
          ]
        }
      ],
      "source": [
        "print(\"Running baseline evaluation...\")\n",
        "baseline_metrics = trainer.evaluate(eval_dataset=eval_dataset)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"BASELINE METRICS\")\n",
        "print(\"=\"*80)\n",
        "for key, value in baseline_metrics.items():\n",
        "    print(f\"{key:40s}: {value}\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef15bb7b",
      "metadata": {},
      "source": [
        "## 9. トレーニング実行\n",
        "\n",
        "モデルをトレーニングします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c10e4956",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Starting training...\\n\")\n",
        "train_result = trainer.train()\n",
        "\n",
        "train_metrics = train_result.metrics\n",
        "train_metrics[\"train_samples\"] = len(train_dataset)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TRAINING COMPLETED\")\n",
        "print(\"=\"*80)\n",
        "for key, value in train_metrics.items():\n",
        "    print(f\"{key:40s}: {value}\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84da2f48",
      "metadata": {},
      "source": [
        "## 10. テストセットでの最終評価"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77a9201b",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Running test evaluation...\")\n",
        "test_metrics = trainer.evaluate(eval_dataset=predict_dataset, metric_key_prefix=\"test\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TEST METRICS\")\n",
        "print(\"=\"*80)\n",
        "for key, value in test_metrics.items():\n",
        "    print(f\"{key:40s}: {value}\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d986b8e5",
      "metadata": {},
      "source": [
        "## 11. 結果の比較\n",
        "\n",
        "ベースライン vs テスト結果を比較します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a51c0aea",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 比較用データ作成\n",
        "comparison_data = []\n",
        "\n",
        "# 主要なメトリクスを抽出\n",
        "for key in baseline_metrics.keys():\n",
        "    if key.startswith('eval_') and not any(x in key for x in ['runtime', 'samples_per_second', 'steps_per_second', 'model_preparation']):\n",
        "        metric_name = key.replace('eval_', '')\n",
        "        test_key = 'test_' + metric_name\n",
        "        \n",
        "        baseline_val = baseline_metrics.get(key, 'N/A')\n",
        "        test_val = test_metrics.get(test_key, 'N/A')\n",
        "        \n",
        "        # 数値なら改善率を計算\n",
        "        if isinstance(baseline_val, (int, float)) and isinstance(test_val, (int, float)):\n",
        "            improvement = ((test_val - baseline_val) / baseline_val * 100) if baseline_val != 0 else 0\n",
        "            comparison_data.append({\n",
        "                'Metric': metric_name,\n",
        "                'Baseline': f\"{baseline_val:.4f}\",\n",
        "                'Test': f\"{test_val:.4f}\",\n",
        "                'Change': f\"{improvement:+.2f}%\"\n",
        "            })\n",
        "        else:\n",
        "            comparison_data.append({\n",
        "                'Metric': metric_name,\n",
        "                'Baseline': str(baseline_val),\n",
        "                'Test': str(test_val),\n",
        "                'Change': 'N/A'\n",
        "            })\n",
        "\n",
        "df = pd.DataFrame(comparison_data)\n",
        "\n",
        "print(\"\\n\" + \"=\"*100)\n",
        "print(\"PERFORMANCE COMPARISON: BASELINE vs TEST\")\n",
        "print(\"=\"*100)\n",
        "print(df.to_string(index=False))\n",
        "print(\"=\"*100)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4725b22c",
      "metadata": {},
      "source": [
        "## 12. モデルの保存（オプション）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f718fd3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# モデル保存\n",
        "save_path = OUTPUT_ROOT / 'ngpt_test_final'\n",
        "save_path.mkdir(exist_ok=True)\n",
        "\n",
        "print(f\"Saving model to {save_path}...\")\n",
        "trainer.save_model(str(save_path))\n",
        "tokenizer.save_pretrained(str(save_path))\n",
        "\n",
        "print(\"\\n✓ Model saved successfully!\")\n",
        "print(f\"Saved to: {save_path}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "my-project",
      "language": "python",
      "name": "my-project"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
